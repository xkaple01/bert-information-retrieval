{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PV211_Term_project_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9fn2YQGSb7G",
        "colab_type": "text"
      },
      "source": [
        "<h2 align=\"center\"><i>Two-stage learning to rank without supervision from relevance labels </i><br></h2>\n",
        "\n",
        "\n",
        "<p><br>In this notebook we will train the <i>\"tiny\"</i> version of <i>state-of-the-art</i> information retrieval model inspired by <i>Google AI</i> blogposts:<br/></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li>\n",
        "<a href=\"https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html\"><i>Transformer-XL</i></a>\n",
        "</li>\n",
        "<li>\n",
        "<a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\"><i>BERT</i></a>\n",
        "</li><br/>\n",
        "</ul></p>\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\"> Our model will consist of 2 stages:<br/></p>\n",
        "<ul>\n",
        "<li>\n",
        "<p style=\"font-size:120%; list-style-type:disc;\">Classical <i>matrix-based</i> ranking method (<i>BM25Plus</i> with <i>default</i> parameters) retrieves the most relevant documents. Retrieval is fast, but may be <i>innacurate</i></p>\n",
        "</li>\n",
        "<li>\n",
        "<p style=\"font-size:120%;\"><i>\"Tiny\" BERT</i> reranks the documents retrieved by the <i>firt-stage</i> algorithm</p><br/>\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "<p style=\"font-size:120%;\">\n",
        "<i>Original BERT</i> was trained during a <i>week</i> using <i>64</i> GPUs on <i>extreamly</i> large text corpuses.<br/><br/><br><br></p>\n",
        "\n",
        "<img src=\"https://github.com/xkaple01/bert-information-retrieval/blob/master/bert_ir_system_gc/bert_detailed.png?raw=1\">\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/><br>Our model will be trained from <i>scratch</i> on a <i>single</i> <i>GeForce GTX 850M</i> GPU.<br/><br/></p>\n",
        "<p align=\"center\" style=\"text-align:center;font-size:120%;\">\n",
        "<b><i>The model will never touch the relevance labels during the training process</i></b><br/><br/><br/>\n",
        "</p> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "inUAfc6TQMVJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "477b0c6e-c897-433e-acee-883654fa622e"
      },
      "source": [
        "! pip install git+https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git@master\n",
        "! git clone https://github.com/xkaple01/bert-information-retrieval.git\n",
        "! pip install nltk\n",
        "! pip install rank_bm25"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git@master\n",
            "  Cloning https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git (to revision master) to /tmp/pip-req-build-txbp9ov2\n",
            "  Running command git clone -q https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git /tmp/pip-req-build-txbp9ov2\n",
            "Requirement already satisfied (use --upgrade to upgrade): pv211-utils==0.1.dev22+g3e6e680 from git+https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git@master in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pv211-utils==0.1.dev22+g3e6e680) (46.1.3)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.6/dist-packages (from pv211-utils==0.1.dev22+g3e6e680) (3.0.1)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from pv211-utils==0.1.dev22+g3e6e680) (4.1.3)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from pv211-utils==0.1.dev22+g3e6e680) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from gspread->pv211-utils==0.1.dev22+g3e6e680) (2.23.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (0.17.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (0.2.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->pv211-utils==0.1.dev22+g3e6e680) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (2.9)\n",
            "Building wheels for collected packages: pv211-utils\n",
            "  Building wheel for pv211-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pv211-utils: filename=pv211_utils-0.1.dev22+g3e6e680-cp36-none-any.whl size=503621 sha256=445fa555b335e4f4ccdef01dc52f66dff19ee0d9c580b3edf7aa59e6d35ab5ff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9enlcqrm/wheels/fc/91/96/1054afbd540b3a8d5913d19a3f34c07b339c0e58340950a781\n",
            "Successfully built pv211-utils\n",
            "fatal: destination path 'bert-information-retrieval' already exists and is not an empty directory.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.6/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.18.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fyAqWIQyINng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "35ff2dea-ec43-44bc-9885-239dcc9ec90a"
      },
      "source": [
        "from pv211_utils.entities import DocumentBase\n",
        "from pv211_utils.entities import QueryBase\n",
        "from pv211_utils.irsystem import IRSystem\n",
        "from pv211_utils.loader import load_documents\n",
        "from pv211_utils.loader import load_queries\n",
        "from pv211_utils.loader import load_judgements\n",
        "from pv211_utils.eval import mean_average_precision\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from rank_bm25 import BM25Plus\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from random import shuffle\n",
        "from collections import OrderedDict\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class Document(DocumentBase):       \n",
        "    def __init__(self, document_id, authors, bibliography, title, body):\n",
        "        super().__init__(document_id, authors, bibliography, title, body)\n",
        "        self.preprocessed = preprocess_text(self.body)\n",
        "\n",
        "        \n",
        "class Query(QueryBase):\n",
        "    def __init__(self, query_id, body):\n",
        "        super().__init__(query_id, body)\n",
        "        self.preprocessed = preprocess_text(self.body)\n",
        "\n",
        "\n",
        "sm_st = SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words('english')) | \\\n",
        "             set(['', 'co', 'eq', 'pr', 'tr', 'rl', 'psf', 'ko', 'la', 'vz', 'plk', 'o', 'etc', 'igy', 'soc',\n",
        "                   'ic', 'ible','ser', 'ing', 'ob', 'feb', 'wkb', 'ao', 'dp', 'tne', 'sr', 'ux', 'som', 'aft', 'con',\n",
        "                   'rev', 'j', 'b', 'p', 'e', 'a', 'sq', 'op', 'er', 'oc', 'ab', 'bc', 'de', 'im', 'fs', 'vs', 'rf',\n",
        "                   'bi','et', 'al', 'th', 'rd', 'nd', 'pb', 'rt', 'rm', 'qn', 'fd', 'qe', 'qm', 'de', 'vas', 'fig',\n",
        "                   'ty', 'tx', 'tz', 'pai', 'ied', 'ref', 'thn', 'jan', 'pre', 'mth', 'nth', 'uhf', 'btu', 'ink', 'rae',\n",
        "                   'ofr', 'n','f', 'dx', 'dy', 'dz', 'x', 'xx', 'y', 'yy', 'z', 'zz', 'q', 's', 'ax', 'cx', 'cf', 'b',\n",
        "                   'du','u', 'r', 'h', 'l', 'jmin','jmjn','viz', 'fl', 'ld', 'dvl', 're', 'tn', 'aec', 'k', 'i', 'ii',\n",
        "                   'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii',\n",
        "                   'xviii', 'xix', 'xx']) \n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    sentences = re.split(r'\\s\\.\\s', text)\n",
        "    prepr_sentences = []\n",
        "    for s in sentences:\n",
        "        word_tokens = list(filter(None, re.split('[\\s,\\(\\)0-9\\'\\:\\$\\*\\;\\?\\\"\\/\\+\\=\\-]|[\\s.*\\.]|(aero|air|aer|super|sub|hyper|ultra|sonic|retro|poly|multi|therm|magnet|hydro|fero|accel|electro|photo|strobo|alumin|anti|less|non|post|cross|eigen|dynam|curv|cylind|linear|off|ellip|compress|molecul|metal|correl|stream|visc|crystall)', s)))\n",
        "        prepr_sentence_tokens = [sm_st.stem(w) for w in word_tokens if not w in stop_words]\n",
        "        if len(prepr_sentence_tokens) != 0:\n",
        "            prepr_sentences.append(prepr_sentence_tokens)\n",
        "    return prepr_sentences\n",
        "\n",
        "\n",
        "def find_max_length(texts):\n",
        "    l=[]\n",
        "    for text in texts.values():\n",
        "        l.append(len(set(flatten_text(text.preprocessed))))\n",
        "    return max(l), np.argmax(l)\n",
        "\n",
        "\n",
        "def flatten_text(text):\n",
        "    return [w for s in text for w in s]\n",
        "        \n",
        "    \n",
        "def create_new_doc_odict():\n",
        "    new_doc_odict = OrderedDict()\n",
        "    cnt = 0\n",
        "    for doc in documents.values():\n",
        "        if doc.body != '':\n",
        "            cnt += 1\n",
        "            new_doc_odict[cnt] = doc\n",
        "    return new_doc_odict\n",
        "    \n",
        "    \n",
        "def count_term_frequencies(texts):\n",
        "    words_fr = {}\n",
        "    for text in texts:\n",
        "        for sentence in text.preprocessed:\n",
        "            for w in sentence:\n",
        "                if w not in words_fr.keys():\n",
        "                    words_fr[w] = 1\n",
        "                else:\n",
        "                    words_fr[w] = words_fr[w] + 1\n",
        "    return words_fr\n",
        "                    \n",
        "\n",
        "def remove_rare_terms_from_words_fr_dict(words_fr):\n",
        "    words_fr_filtered = {}\n",
        "    for w, fr in words_fr.items():\n",
        "        if fr > 1:\n",
        "            words_fr_filtered[w] = fr\n",
        "    return words_fr_filtered\n",
        "\n",
        "\n",
        "def filter_text(text, words):\n",
        "    filtered_text = []\n",
        "    for s in text:\n",
        "        filtered_sentence = []\n",
        "        for w in s:\n",
        "            if w not in words:\n",
        "                filtered_sentence.append(w)\n",
        "        if len(filtered_sentence)>0:\n",
        "            filtered_text.append(filtered_sentence)\n",
        "    return filtered_text\n",
        "\n",
        "\n",
        "def create_words_to_ids_dict():\n",
        "    words_to_ids = {}\n",
        "    words_fr = count_term_frequencies(list(documents.values()) + list(queries.values()))\n",
        "    words_fr = remove_rare_terms_from_words_fr_dict(words_fr)\n",
        "    unique_terms = gather_unique_terms(words_fr)    \n",
        "    words_to_ids = {'CLS':0, 'SEP':1}    \n",
        "    cnt = 2\n",
        "    for w in sorted(unique_terms):\n",
        "        words_to_ids[w] = cnt\n",
        "        cnt+=1\n",
        "    return words_to_ids, sorted(list(unique_terms))\n",
        "\n",
        "\n",
        "def gather_unique_terms(words_fr):\n",
        "    unique_terms = gather_unique_terms_from_text(queries)\n",
        "    remove_duplicates(documents, unique_terms, words_fr)\n",
        "    remove_duplicates(queries, unique_terms, words_fr)\n",
        "    return set(unique_terms) & set(words_fr.keys())\n",
        "\n",
        "\n",
        "def gather_unique_terms_from_text(texts):\n",
        "    unique_terms = []\n",
        "    for t in texts.values():\n",
        "        for s in t.preprocessed:\n",
        "            for w in s:\n",
        "                if w not in unique_terms:\n",
        "                    unique_terms.append(w)\n",
        "    return unique_terms\n",
        "\n",
        "\n",
        "def remove_duplicates(texts, unique_terms, words_fr):\n",
        "    for t in texts.values():\n",
        "        filtered_text = []\n",
        "        for s in t.preprocessed:\n",
        "            filtered_sentence = []\n",
        "            for w in s:\n",
        "                if w in unique_terms and w in words_fr.keys():\n",
        "                    filtered_sentence.append(w)\n",
        "            if len(filtered_sentence)>0:\n",
        "                filtered_text.append(filtered_sentence)\n",
        "        t.preprocessed = filtered_text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBBjJiQ0Sb7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "57724ce1-5974-41f5-fe52-763bda4f4aa3"
      },
      "source": [
        "documents = load_documents(Document)\n",
        "queries = load_queries(Query)\n",
        "relevant = load_judgements(queries, documents)\n",
        "\n",
        "documents = create_new_doc_odict()\n",
        "words_to_ids, unique_terms = create_words_to_ids_dict()\n",
        "vocab_size = len(words_to_ids.keys())\n",
        "num_docs = len(documents.values())\n",
        "num_queries = len(queries.values())\n",
        "\n",
        "print('Documents: ', num_docs)\n",
        "print('Queries: ', num_queries)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documents:  1398\n",
            "Queries:  225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25jYyse3Sb7f",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<h3 align=\"center\" style=\"text-align:center;\"><i>First-stage retrieval algorithm</i><br/><br></h3>\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/><i>Preprosessing</i> pipeline includes following steps:<br/><br/></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li>Text is splitted into sentences, sentences are splitted into the tokens, <i>stopwords</i> are removed and each token is <i>stemmed</i> by <i>SnowballStemmer</i><br/><br/></li>\n",
        "<li><i>Relevance</i> scores between <i>query</i> and <i>each document</i> are calculated using <i>BM25Plus</i> algorithm with <i>default</i> configuration (no explicit optimization on hyperparameters). The most <i>relevant documents</i> appear at the top of retrieved list of documents. Howewer, the <i>order</i> of documents may be <i>inacurate.</i></li></ul><br>\n",
        "\n",
        "\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ2vlEaqSb7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BM25IRSystem(IRSystem):\n",
        "    def __init__(self):\n",
        "        self.documents = list(documents.values())\n",
        "        self.corpus = [flatten_text(doc.preprocessed) for doc in self.documents]\n",
        "        self.bm25 = BM25Plus(self.corpus)\n",
        "\n",
        "        \n",
        "    def first_stage_ranking(self, preprocessed_query):\n",
        "        scores = self.bm25.get_scores(preprocessed_query.preprocessed[0])\n",
        "        doc_list_ids = np.argsort(scores)[::-1]\n",
        "        return np.take(self.documents, doc_list_ids, axis=0).tolist()[:]\n",
        "\n",
        "                \n",
        "    def search(self, query):\n",
        "        preprocessed_query = queries[query.query_id]\n",
        "        fs_ranked_docs = self.first_stage_ranking(preprocessed_query)\n",
        "        return fs_ranked_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1OzfxtUSb7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7e15fbda-4abe-4712-e422-56d0deb16704"
      },
      "source": [
        "mean_average_precision(BM25IRSystem(), submit_result=False, author_name=\"Kaplenko, Mykola\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean average precision: 42.703% \n",
            "Not submitted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMpukwRkSb7w",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<h3 align=\"center\" style=\"text-align:center;\"><i>Now, let's train the second-stage reranking model: \"tiny\" BERT</i><br/><br></h3>\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/>    \n",
        "Obviously, reranking models are trained in a <i>supervised</i> manner using the <i>relevance labels</i>: model takes <i>2</i> documents as inputs (one document is <i>relevant</i>, another document is always <i>non-relevant</i>), model then tries to guess which of <i>2</i> input documents is <i>relevant</i>.<br/><br/></p>\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/>  \n",
        "We <b>will not use</b> the <i>relevance labels</i>. Instead, we slightly reformulate our task in order to train the model without any supervision from the <i>relevance labels</i>.<br/><br/></p>\n",
        "\n",
        "<p style=\"font-size:120%;\"> The <i>key idea</i> is to train our model in a such a way that it will develop the <i>general skill</i> of languge understanding.<br/><br/><br/>\n",
        "Training process is organized as follows:<br/><br/></p>\n",
        "<ul style=\"font-size:120%;\"><li>\n",
        "We randomly choose <i>2</i> documents from the list of documents. Randomly extract several (let's say <i>N</i>) words from the <i>first document</i> and randomly remove <i>K</i> words from the <i>second document</i> such that the documents now have the <i>same</i> length<br/><br/></li>\n",
        "<li>Model takes <i>3</i> inputs: the <i>extracted words</i>, the <i>first document</i> (without <i>extracted words</i>) and the <i>second document</i> (from which K words were removed)</li>\n",
        "<br/>\n",
        "<li>Model tries to answer the <i>question</i>: do <i>extracted words</i> belong to the <i>first document</i> or to the <i>second one</i>? We know from which one document we <i>extracted</i> the words, so we can train the model in the <i>supervised</i> manner, but we <b>do not use</b> the <i>relevance labels</i><br/><br/></li></ul>\n",
        "\n",
        "<p style=\"font-size:120%;\">\n",
        "To <i>succesfully</i> perform its task, the model has to learn the <i>relations</i> between the <i>context</i> and the <i>words</i>, whose <i>meaning</i> depends on this <i>context</i>.<br/><br/></p>\n",
        "\n",
        "<img src=\"https://github.com/xkaple01/bert-information-retrieval/blob/master/bert_ir_system_gc/multiple_choice_2.png?raw=1\" align=\"middle\">\n",
        "\n",
        "<br>\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/>\n",
        "But <i><b>how</b></i> can we use this to rerank documents?<br/><br/></p>\n",
        "\n",
        "<p style=\"font-size:120%;\">\n",
        "Trained model again takes <i>3</i> inputs: <i>query</i>, <i>whole firs document</i> (with no <i>words extracted</i>), and <i>whole second document</i>. Model then <i>predicts</i>: is it more likely that the <i>query words</i> originate (\"<i>were extracted</i>\") from the <i>first document</i> or from the <i>second document</i>?<br/><br/> Documents in the list of retrieved documents are then <i>pairwise</i> rearanged in such a way, that the more relevant documents occur in the list at higher positions than the less relevant documents.<br/><br></p>\n",
        "\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlgND5xzSb7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "3a6ffe3c-1a08-442d-b19b-79411cf0d6fc"
      },
      "source": [
        "import tensorflow as tf\n",
        "! pip install transformers\n",
        "from transformers import *\n",
        "\n",
        "PATH_TO_LOGS = './logs'\n",
        "PATH_TO_MODEL = './model/model_checkpoint' \n",
        "\n",
        "def create_directory(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6LJis47Sb79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "483346b0-cfda-4791-af51-05febc1be622"
      },
      "source": [
        "documents = load_documents(Document)\n",
        "queries = load_queries(Query)\n",
        "relevant = load_judgements(queries, documents)\n",
        "\n",
        "documents = create_new_doc_odict()\n",
        "words_to_ids, unique_terms = create_words_to_ids_dict()\n",
        "vocab_size = len(words_to_ids.keys())\n",
        "num_docs = len(documents.values())\n",
        "num_queries = len(queries.values())\n",
        "\n",
        "print('Documents: ', num_docs)\n",
        "print('Queries: ', num_queries)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documents:  1398\n",
            "Queries:  225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRY7z0_dSb8E",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<h3 align=\"center\"><i>Let's create the data generators:</i><br/><br></h3>\n",
        "<p style=\"font-size:120%;\">Note, the <i>TrainDataGenerator</i> never touches the relevance pairs (query - relevant document) for the training.\n",
        "Relevance labels are only used in <i>ValidationDataGenerator</i> to monitor the model performance during the training and <b>does not affect</b> the training process.</p>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmUlugcpSb8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainDataGenerator(tf.keras.utils.Sequence):   \n",
        "    def __len__(self):\n",
        "        return 300\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_x_input_ids = []\n",
        "        data_x_attention_masks = []\n",
        "        data_x_token_type_ids = []\n",
        "        data_x_position_ids = []\n",
        "        data_y = []\n",
        "        for _ in range(BATCH_SIZE):\n",
        "            while True:                  \n",
        "                random_doc_ids = np.random.choice(num_docs, 2 , replace=False) + 1\n",
        "                random_docs = np.array([documents[random_doc_ids[0]], documents[random_doc_ids[1]]])\n",
        "                if not bool(set(random_docs[0].preprocessed[0]) & set(random_docs[1].preprocessed[0])):\n",
        "                    break\n",
        "                                \n",
        "            polarity = np.random.choice(2, 2, replace=False)\n",
        "            p_doc = random_docs[0]\n",
        "            n_doc = random_docs[1]\n",
        "            \n",
        "            p_random_words, p_words, n_words = select_random_words_from_texts(p_doc.preprocessed, n_doc.preprocessed)\n",
        "           \n",
        "            text1 = p_random_words\n",
        "            p_text = p_words\n",
        "            n_text = n_words\n",
        "\n",
        "            n_p_texts = np.array([n_text, p_text])\n",
        "            random_texts = np.take(n_p_texts, polarity, axis=0).tolist()\n",
        "            text2 = random_texts[0]\n",
        "            text3 = random_texts[1]\n",
        "\n",
        "            sample_input_ids, sample_attention_mask, sample_token_type_ids, sample_position_ids = create_sample(text1, text2, text3)\n",
        "            label = polarity\n",
        "            \n",
        "            data_x_input_ids.append(sample_input_ids)\n",
        "            data_x_attention_masks.append(sample_attention_mask)\n",
        "            data_x_token_type_ids.append(sample_token_type_ids)\n",
        "            data_x_position_ids.append(sample_position_ids)\n",
        "            data_y.append(label)\n",
        "            \n",
        "        return ([np.array(data_x_input_ids), np.array(data_x_attention_masks), np.array(data_x_token_type_ids), np.array(data_x_position_ids)], np.array(data_y))\n",
        "\n",
        "\n",
        "    \n",
        "class ValidationDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self):\n",
        "        self.rel_pairs = list(relevant)\n",
        "        self.num_rel_pairs = len(self.rel_pairs)\n",
        "        self.len = 25\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_x_input_ids = []\n",
        "        data_x_attention_masks = []\n",
        "        data_x_token_type_ids = []\n",
        "        data_x_sample_position_ids = []\n",
        "        data_y = []\n",
        "        for _ in range(BATCH_SIZE):\n",
        "            random_pair_idx = np.random.randint(self.num_rel_pairs, size=1).item()\n",
        "            random_pair = self.rel_pairs[random_pair_idx]\n",
        "            polarity = np.random.choice(2, 2, replace=False)\n",
        "              \n",
        "            while True:\n",
        "                n_random_doc_id = np.random.randint(num_docs, size=1).item() + 1\n",
        "                if not (random_pair[0], documents[n_random_doc_id]) in relevant:\n",
        "                    break\n",
        "\n",
        "            p_text = random_pair[1].preprocessed  \n",
        "            n_text = documents[n_random_doc_id].preprocessed\n",
        "        \n",
        "            p_random_words, p_words, n_words = select_random_words_from_texts(p_text, n_text)\n",
        "           \n",
        "            text1 = p_random_words\n",
        "            p_text = p_words\n",
        "            n_text = n_words \n",
        "        \n",
        "            n_p_texts = np.array([n_text, p_text])\n",
        "            random_texts = np.take(n_p_texts, polarity, axis=0).tolist()\n",
        "            text2 = random_texts[0]\n",
        "            text3 = random_texts[1]\n",
        "            \n",
        "            sample_input_ids, sample_attention_mask, sample_token_type_ids, sample_position_ids = create_sample(text1, text2, text3)\n",
        "            label = polarity\n",
        "            \n",
        "            data_x_input_ids.append(sample_input_ids)\n",
        "            data_x_attention_masks.append(sample_attention_mask)\n",
        "            data_x_token_type_ids.append(sample_token_type_ids)\n",
        "            data_x_sample_position_ids.append(sample_position_ids)\n",
        "            data_y.append(label)\n",
        "            \n",
        "        return ([np.array(data_x_input_ids), np.array(data_x_attention_masks), np.array(data_x_token_type_ids), np.array(data_x_sample_position_ids)], np.array(data_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehZXAWu1Sb8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sample_part(text, token_type_id, position_id):\n",
        "    set(flatten_text(text))\n",
        "    words = []\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    token_type_ids = []\n",
        "    position_ids = []\n",
        "    for sentence in text:\n",
        "        for w in sentence:\n",
        "            if w in words_to_ids.keys():\n",
        "                words.append(w)\n",
        "                input_ids.append(words_to_ids[w])\n",
        "                attention_mask.append(1)\n",
        "                token_type_ids.append(token_type_id)\n",
        "                position_ids.append(position_id)\n",
        "    words.append('SEP')\n",
        "    input_ids.append(words_to_ids['SEP'])\n",
        "    attention_mask.append(1)\n",
        "    token_type_ids.append(token_type_id)\n",
        "    position_ids.append(position_id)\n",
        "    return words, input_ids, attention_mask, token_type_ids, position_ids\n",
        "\n",
        "\n",
        "def create_sample(text1, text2, text3):\n",
        "    sample_words = ['CLS']\n",
        "    sample_input_ids = [words_to_ids['CLS']]\n",
        "    sample_attention_mask = [1]\n",
        "    sample_token_type_ids = [0]\n",
        "    sample_position_ids = [0]\n",
        "\n",
        "    part1 = create_sample_part(text1, token_type_id=0, position_id=0)\n",
        "    part2 = create_sample_part(text2, token_type_id=1, position_id=0)\n",
        "    part3 = create_sample_part(text3, token_type_id=2, position_id=0)\n",
        "    \n",
        "    n_pad_1 = BERT_MAX_SEQ_LEN - (len(part1[0]) + len(part2[0]) + 1)\n",
        "    n_pad_2 = BERT_MAX_SEQ_LEN - (len(part1[0]) + len(part3[0]) + 1)\n",
        "\n",
        "    zero_pad_1 = np.zeros(n_pad_1, dtype='int32')\n",
        "    zero_pad_2 = np.zeros(n_pad_2, dtype='int32')\n",
        "    \n",
        "    sample_words_1 = np.concatenate([np.array(sample_words + part1[0] + part2[0]), zero_pad_1])\n",
        "    sample_input_ids_1 = np.concatenate([np.array(sample_input_ids + part1[1] + part2[1]), zero_pad_1])\n",
        "    sample_attention_mask_1 = np.concatenate([np.array(sample_attention_mask + part1[2] + part2[2]), zero_pad_1])\n",
        "    sample_token_type_ids_1 = np.concatenate([np.array(sample_token_type_ids + part1[3] + part2[3]), zero_pad_1])\n",
        "    sample_position_ids_1 = np.concatenate([np.array(sample_position_ids + part1[4] + part2[4]), zero_pad_1])\n",
        "    \n",
        "    sample_words_2 = np.concatenate([np.array(sample_words + part1[0] + part3[0]), zero_pad_2])\n",
        "    sample_input_ids_2 = np.concatenate([np.array(sample_input_ids + part1[1] + part3[1]), zero_pad_2])\n",
        "    sample_attention_mask_2 = np.concatenate([np.array(sample_attention_mask + part1[2] + part3[2]), zero_pad_2])\n",
        "    sample_token_type_ids_2 = np.concatenate([np.array(sample_token_type_ids + part1[3] + part3[3]), zero_pad_2])\n",
        "    sample_position_ids_2 = np.concatenate([np.array(sample_position_ids + part1[4] + part3[4]), zero_pad_2])\n",
        "    \n",
        "    sample_words = np.stack([sample_words_1, sample_words_2])\n",
        "    sample_input_ids = np.stack([sample_input_ids_1, sample_input_ids_2])\n",
        "    sample_attention_mask = np.stack([sample_attention_mask_1, sample_attention_mask_2])\n",
        "    sample_token_type_ids = np.stack([sample_token_type_ids_1, sample_token_type_ids_2])\n",
        "    sample_position_ids = np.stack([sample_position_ids_1, sample_position_ids_2])\n",
        "        \n",
        "    return sample_input_ids, sample_attention_mask, sample_token_type_ids, sample_position_ids\n",
        "\n",
        "\n",
        "def select_random_words_from_texts(p_text, n_text):\n",
        "    num_words_to_select = np.random.choice(range(MIN_NUM_WORDS_TO_SELECT, MAX_NUM_WORDS_TO_SELECT), 1).item()\n",
        "    p_words = flatten_text(p_text)\n",
        "    n_words = flatten_text(n_text)\n",
        "    \n",
        "    p_n_words = list(set(p_words) & set(n_words))\n",
        "    p_unique_words = list(set(p_words) - set(p_n_words))\n",
        "    n_unique_words = list(set(n_words) - set(p_n_words))\n",
        "    num_words_to_select = np.min([num_words_to_select, len(p_unique_words)//2, len(n_unique_words)//2])\n",
        "\n",
        "    shuffle(p_unique_words)\n",
        "    p_random_words = p_unique_words[:num_words_to_select]\n",
        "    \n",
        "    shuffle(n_unique_words)\n",
        "    n_random_words = n_unique_words[:num_words_to_select]\n",
        "\n",
        "    p_filtered_text = filter_text(p_text, p_random_words)\n",
        "    p_filtered_text_fl = flatten_text(p_filtered_text)\n",
        "    shuffle(p_filtered_text_fl)\n",
        "    p_words = p_filtered_text_fl[:num_words_to_select]\n",
        "      \n",
        "    return [p_random_words], [p_words], [n_random_words]\n",
        "\n",
        "\n",
        "\n",
        "def prepare_doc_batch(query, docs):\n",
        "    data_x_input_ids = []\n",
        "    data_x_attention_masks = []\n",
        "    data_x_token_type_ids = []\n",
        "    data_x_sample_position_ids = []\n",
        "    \n",
        "    for i in range(0, len(docs)-1, 2):             \n",
        "        p_words = flatten_text(docs[i].preprocessed)\n",
        "        n_words = flatten_text(docs[i+1].preprocessed)\n",
        "    \n",
        "        p_n_words = list(set(p_words) & set(n_words))\n",
        "        p_unique_words = list(set(p_words) - set(p_n_words))\n",
        "        n_unique_words = list(set(n_words) - set(p_n_words))\n",
        "          \n",
        "        text1 = query.preprocessed\n",
        "        text2 = [p_unique_words]\n",
        "        text3 = [n_unique_words]\n",
        "        \n",
        "        sample_input_ids, sample_attention_mask, sample_token_type_ids, sample_position_ids = create_sample(text1, text2, text3)\n",
        "        \n",
        "        data_x_input_ids.append(sample_input_ids)\n",
        "        data_x_attention_masks.append(sample_attention_mask)\n",
        "        data_x_token_type_ids.append(sample_token_type_ids)\n",
        "        data_x_sample_position_ids.append(sample_position_ids)\n",
        "        \n",
        "    return [np.array(data_x_input_ids), np.array(data_x_attention_masks), np.array(data_x_token_type_ids), np.array(data_x_sample_position_ids)]\n",
        "      \n",
        "    \n",
        "    \n",
        "def swap_docs_based_on_predictions(raw_doc_batch, predictions):\n",
        "    reranked_docs = raw_doc_batch   \n",
        "    for i in range(predictions.shape[0]):\n",
        "        if predictions[i][0] + 0.2 < predictions[i][1]:\n",
        "            t = reranked_docs[2*i]\n",
        "            reranked_docs[2*i] = reranked_docs[2*i+1]\n",
        "            reranked_docs[2*i+1] = t            \n",
        "    return reranked_docs\n",
        "        \n",
        "    \n",
        "    \n",
        "def rerank_docs_based_on_query(query, fs_ranked_documents):   \n",
        "    docs_to_rerank = fs_ranked_documents[4:44]\n",
        "    prepared_batch = prepare_doc_batch(query, docs_to_rerank)\n",
        "    predictions = model.predict(prepared_batch, batch_size=BATCH_SIZE)\n",
        "    reranked_docs = swap_docs_based_on_predictions(docs_to_rerank, predictions)\n",
        "    fs_ranked_documents[4:44] = reranked_docs \n",
        "    return fs_ranked_documents  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQGusX74Sb8T",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<h3 align=\"center\"><i>Now, the most interesting part - we define the model:</i><br/><br></h3>\n",
        "<p style=\"font-size:120%;\">Model's inputs have the following structure: <br/><br></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li><i>input_ids</i>: each token has its unique <i>id</i>, this <i>id</i> is used in the <i>BERT</i> <i>embedding layer</i> to hash the integer index to the numeric vector representing the token as the float-pointing numbers</li><br/>\n",
        "<li><i>attention_mask</i>: model always takes the input of predefined length; if the documents are shorter than this predefined length - they are padded with zeros; attention mask then indicates which tokens belong to the documents and which tokens (padded zeros) were appended to match the required input length</li><br/>\n",
        "<li><i>token_type_ids</i>: indicate whether token correnspond to the first or to the second input document</li><br/>\n",
        "<li><i>position_ids</i>: encode the order of words in documents</li><br/></ul></p>\n",
        "\n",
        "<p style=\"font-size:120%;\">\n",
        "Model predicts <i>2</i> softmax scores: probability that the words were extracted from the first document and the probability that the words were extraceted from the second one<br><br></p>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjI4fwc9Sb8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(bert_config):\n",
        "    num_choices = 2\n",
        "    bert = TFBertForMultipleChoice(bert_config)\n",
        "    input_ids = tf.keras.Input(shape=(num_choices, BERT_MAX_SEQ_LEN,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = tf.keras.Input(shape=(num_choices, BERT_MAX_SEQ_LEN,), dtype=tf.int32, name='attention_mask')\n",
        "    token_type_ids = tf.keras.Input(shape=(num_choices, BERT_MAX_SEQ_LEN,), dtype=tf.int32, name='token_type_ids')\n",
        "    position_ids = tf.keras.Input(shape=(num_choices, BERT_MAX_SEQ_LEN,), dtype=tf.int32, name='position_ids')\n",
        "    \n",
        "    \n",
        "    logits = bert([input_ids, attention_mask, token_type_ids, position_ids])[0]\n",
        "    output = tf.keras.layers.Softmax(input_shape=(num_choices,))(logits)\n",
        "    model = tf.keras.Model([input_ids, attention_mask, token_type_ids, position_ids], output)\n",
        "    \n",
        "    \n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksN1wWM_Sb8c",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<h3 align=\"center\"><i>Configure the model for training:<br><br></i></h3>\n",
        "<p style=\"font-size:120%;\">Note, that we train the model from <i>scratch</i>. Training process may be unstable, so:<br><br></p>\n",
        "<ul style=\"font-size:120%;\"> \n",
        "<li>Use the batch size at least <i>128</i> (if the batch <i>does not</i> fit to memory - you will get the <i>OOM error</i> - just <i>decrease</i> the batch size)</li><br/>\n",
        "<li>Use the <i>lower</i> learning rate (<i>i.e. 3e-6</i>)</li><br/><br/></ul>\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\">You can experiment with different settings: <br/><br></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li>Increase the batch size $n$ times means the reduction of <i>gradient variance</i> by factor $\\sqrt{n}$. In other words, we <i>do not</i> benefit from the <i>too high</i> batch sizes - they still <i>do not</i> provide the true estimation of <i>gradient</i>, but significantly <i>slow down</i> the training. On the other hand, with the <i>too small</i> batch size the model can simply <i>not to converge</i> at all</li><br/>\n",
        "<li>If you decrese the batch size $n$ times, you have to decrese the learning rate by factor $\\sqrt{n}$ to preserve the stability of learning process</li><br/>\n",
        "<li><i>Epochs</i> is just an arbitrary high number. Model weights are saved after <i>each</i> epoch through the <i>ModelCheckpoint</i> callback, so feel free to stop the learning whenever you want (when the model will be <i>precise</i> enough)<br><br><br></li></ul>\n",
        "    \n",
        "    \n",
        "<p style=\"font-size:120%;\">Model whose performance is reported below was trained with the <i>initial</i> learning rate <i>1e-5</i> to achieve the faster convergence at the <i>initial</i> phase of training. Then, the learning rate was decreased to <i>3e-6</i> and the network continued to train during the <i>14</i> additional hours. The whole training process took approximately <i>18</i> hours (on the <i>weeny</i> but <i>proud</i> <i>GeForce GTX 850M</i> GPU). <br><br> Model has only <i>65k</i> parameters. Experiments with <i>higher learning rates</i> led to the model <i>divergence</i>; <i>smaller batch sizes</i> (<i>i.e. 8</i>) - <i>convergence</i> to the <i>local minimums</i>, <i>performans degradation</i> and the <i>disability</i> to return to the normal training process.<br><br></p>\n",
        "    \n",
        "    \n",
        "<p style=\"font-size:120%;\">Performance in the final phase of training: <br><br></p>\n",
        "\n",
        "<img src=\"https://github.com/xkaple01/bert-information-retrieval/blob/master/bert_ir_system_gc/learning_process.gif?raw=1\" align=\"middle\">\n",
        "<br><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb7vRJQUSb8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.000003\n",
        "EPOCHS = 1000\n",
        "MIN_NUM_WORDS_TO_SELECT = 75\n",
        "MAX_NUM_WORDS_TO_SELECT = 76 \n",
        "BERT_MAX_SEQ_LEN = 25 + MAX_NUM_WORDS_TO_SELECT + 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twiLYulcSb8k",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<h3><i>Original BERT:</i></h3><br>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li><i>12</i> attention layers</li><br/>\n",
        "<li>Each token (english has approximately <i>30000</i> of the most commonly used words) is encoded as a vecor of <i>512</i> numbers</li><br/>\n",
        "<li>Memory size (dimensionality of pointwise dense layers in encoder blocks) is <i>3072</i></li></ul><br/>\n",
        "<p style=\"font-size:120%;\">As a result, the model requires the <i>gigabytes</i> of textual data, <i>64</i> GPUs and a <i>week</i> of training. <br/><br/> We will design the <i>\"tiny\"</i> version of <i>BERT</i>. The model has to be powerful enough to capture all the hidden relations between words and at the same time has not to be overparametrized in order not to overfit hardly.</p><br/><br/>\n",
        "    \n",
        "<p style=\"font-size:120%;\">Recommended settings are following:<br/><br/></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li><i>2</i> (but better at least <i>3</i> attention layers - this, ofcourse, will slow down the training and increase the inference time)</li><br/>\n",
        "<li>Hidden size is <i>32</i>: each token in our scientific Cranfield language will be represented as a vector of <i>32</i> numbers</li><br/>\n",
        "<li>Intermediate (memory size) is at least <i>256</i>: we want to reliably capture the hidden relations between words</li><br/>\n",
        "<li>Number of attention heads is just <i>1</i>. The dot products between such a small (<i>32</i>-dim) representations do not require the vector to be splitted into several parts and to be attended by multiple heads. But feel free to set the number of attention heads to <i>2</i> or <i>4</i></li>\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5fglg8-Sb8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_train_config = BertConfig(vocab_size=680, hidden_size=32, num_hidden_layers=2, num_attention_heads=1, intermediate_size=256, max_position_embeddings=3, type_vocab_size=3, training=True)\n",
        "\n",
        "train_data_generator = TrainDataGenerator()\n",
        "val_data_generator = ValidationDataGenerator()\n",
        "\n",
        "model = create_model(bert_train_config)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=PATH_TO_LOGS, histogram_freq=1, write_graph=True, write_images=False)\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=PATH_TO_MODEL, monitor='accuracy', verbose=1, save_best_only=False, save_weights_only=True, save_freq='epoch')\n",
        "\n",
        "\n",
        "history = model.fit(x=train_data_generator, validation_data = val_data_generator, epochs=EPOCHS, callbacks=[tb_callback, ckpt_callback])\n",
        "model.save_weights(PATH_TO_MODEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehj-4AELSb8t",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<p style=\"font-size:120%;\">Note how the <i>validation loss</i> is <i>improved</i> as the training proceeds. The increasing <i>language understanding general ability</i> of the model leads to improvement in the reranking accuracy even though the model <b>does not have</b> any information about the <i>query - document</i> relevance.</p><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh3qp4DmSb8u",
        "colab_type": "text"
      },
      "source": [
        "<h3 align=\"center\"><i>It's time to test our model:</i><br/><br></h3>\n",
        "<p style=\"font-size:120%;\">Let's configure the inference phase:</p><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQG2_LSCSb8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 1\n",
        "BERT_MAX_SEQ_LEN = 512 \n",
        "bert_inference_config = BertConfig(vocab_size=680, hidden_size=32, num_hidden_layers=2, num_attention_heads=1, intermediate_size=256, max_position_embeddings=3, type_vocab_size=3, training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr5mlHAJSb84",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<p style=\"font-size:120%;\">Alternatively, you can load the trained model in order to reproduce the final achieved result reported below</p><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04vgawW8Sb86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "7ef79a71-1101-4f5a-bf15-91fff5655f63"
      },
      "source": [
        "PATH_TO_MODEL  = '/content/bert-information-retrieval/bert_ir_system_gc/trained_model/model_checkpoint'\n",
        "model = create_model(bert_inference_config)\n",
        "model.load_weights(PATH_TO_MODEL)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 2, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_mask (InputLayer)     [(None, 2, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 2, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "position_ids (InputLayer)       [(None, 2, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_multiple_choice_1 ( ((None, 2),)         65153       input_ids[0][0]                  \n",
            "                                                                 attention_mask[0][0]             \n",
            "                                                                 token_type_ids[0][0]             \n",
            "                                                                 position_ids[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (None, 2)            0           tf_bert_for_multiple_choice_1[0][\n",
            "==================================================================================================\n",
            "Total params: 65,153\n",
            "Trainable params: 65,153\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd7c41bdfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p62UPjpSb9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.000003\n",
        "EPOCHS = 1000\n",
        "MIN_NUM_WORDS_TO_SELECT = 75\n",
        "MAX_NUM_WORDS_TO_SELECT = 76 \n",
        "BERT_MAX_SEQ_LEN = 25 + MAX_NUM_WORDS_TO_SELECT + 4\n",
        "PATH_TO_MODEL  = '/content/bert-information-retrieval/bert_ir_system_gc/trained_model/model_checkpoint'\n",
        "bert_train_config = BertConfig(vocab_size=680, hidden_size=32, num_hidden_layers=2, num_attention_heads=1, intermediate_size=256, max_position_embeddings=3, type_vocab_size=3, training=True)\n",
        "\n",
        "train_data_generator = TrainDataGenerator()\n",
        "val_data_generator = ValidationDataGenerator()\n",
        "\n",
        "model = create_model(bert_train_config)\n",
        "model.load_weights(PATH_TO_MODEL)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=PATH_TO_LOGS, histogram_freq=1, write_graph=True, write_images=False)\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=PATH_TO_MODEL, monitor='accuracy', verbose=1, save_best_only=False, save_weights_only=True, save_freq='epoch')\n",
        "\n",
        "\n",
        "history = model.fit(x=train_data_generator, validation_data = val_data_generator, epochs=EPOCHS, callbacks=[tb_callback, ckpt_callback])\n",
        "model.save_weights(PATH_TO_MODEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY67TryWSb9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertIRSystem(IRSystem):\n",
        "    def __init__(self):\n",
        "        self.documents = list(documents.values())\n",
        "        self.corpus = [flatten_text(doc.preprocessed) for doc in self.documents]\n",
        "        self.bm25 = BM25Plus(self.corpus)\n",
        "\n",
        "        \n",
        "    def first_stage_ranking(self, preprocessed_query):\n",
        "        scores = self.bm25.get_scores(preprocessed_query.preprocessed[0])\n",
        "        doc_list_ids = np.argsort(scores)[::-1]\n",
        "        return np.take(self.documents, doc_list_ids, axis=0).tolist()\n",
        "\n",
        "                \n",
        "    def search(self, query):\n",
        "        preprocessed_query = queries[query.query_id]\n",
        "        fs_ranked_docs = self.first_stage_ranking(preprocessed_query)\n",
        "\n",
        "        reranked_docs = rerank_docs_based_on_query(preprocessed_query, fs_ranked_docs)\n",
        "            \n",
        "        return reranked_docs  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHCd85-vSb9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6bdef141-ff41-4a98-be50-2cfd613d218e"
      },
      "source": [
        "mean_average_precision(BertIRSystem(), submit_result=False, author_name=\"Kaplenko, Mykola\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean average precision: 42.761% \n",
            "Not submitted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2QkPE31Sb9V",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<p style=\"font-size:120%;\">Note the increase in MAP after the document reranking by our second-stage model. The increase is not too high but:<br/><br/>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li>The reranking was applied only once to pairs of neighbouring documents (to achieve the reasonable query response time). The higher number of pairwise rerankings significantly slows down the document retrieval</li><br/>\n",
        "<li>Model was trained from scratch. We did no fine-tuning and did not use any huge model pretrained by <i>Google</i> on <i>gigabytes</i> of data</li><br/> \n",
        "<li>Our model was trained on general language understanding task (i.e. document prediction based on extracted words) and have never used the relevance labels. In contrast, the real information retrieval systems always use the extreamly huge datasets of queries and the human-defined document relevance labels</li></ul>\n",
        "<br>\n",
        "<p style=\"font-size:120%;\">It will be interesting to change the project slightly in the following years by providing the train and the validation dataset such that we can train our models for the primary tasks they will be used for.</p><br>\n",
        "<hr>\n",
        "\n",
        "<br/>\n",
        "<p align=\"center\" style=\"font-size:120%;text-align:center;\">Thank you for <i>attention</i>. I hope you have enjoyed our small information retrieval challenge.<br/><br>"
      ]
    }
  ]
}