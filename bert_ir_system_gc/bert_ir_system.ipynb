{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PV211_Term_project_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9fn2YQGSb7G",
        "colab_type": "text"
      },
      "source": [
        "<h2 align=\"center\"><i>Two-stage learning to rank without supervision from relevance labels </i><br></h2>\n",
        "<hr>\n",
        "\n",
        "<p><br>In this notebook we will train the <i>\"tiny\"</i> version of <i>state-of-the-art</i> information retrieval model inspired by <i>Google AI</i> blogposts:<br/></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li>\n",
        "<a href=\"https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html\">Transformer-XL</a>\n",
        "</li>\n",
        "<li>\n",
        "<a href=\"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\">BERT</a>\n",
        "</li><br/>\n",
        "</ul></p>\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\"> Our model will consist of 2 stages:<br/></p>\n",
        "<ul>\n",
        "<li>\n",
        "<p style=\"font-size:120%; list-style-type:disc;\">Classical <i>matrix-based</i> ranking method (<i>BM25Plus</i> with <i>default</i> parameters) retrieves the most relevant documents. Retrieval is fast, but may be <i>innacurate</i></p>\n",
        "</li>\n",
        "<li>\n",
        "<p style=\"font-size:120%;\"><i>\"Tiny\" BERT</i> reranks the documents retrieved by the <i>firt-stage</i> algorithm</p><br/>\n",
        "</li>\n",
        "</ul>\n",
        "\n",
        "<p style=\"font-size:120%;\">\n",
        "<i>Original BERT</i> was trained during a <i>week</i> using <i>64</i> GPUs on <i>extreamly</i> large text corpuses.<br/><br/><br><br></p>\n",
        "\n",
        "<img src=\"https://github.com/xkaple01/bert-information-retrieval/blob/master/bert_ir_system_gc/bert_detailed.png?raw=1\">\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/><br>Our model will be trained from <i>scratch</i> on a <i>single</i> <i>GeForce GTX 850M</i> GPU.<br/><br/></p>\n",
        "<p align=\"center\" style=\"text-align:center;font-size:120%;\">\n",
        "<b><i>The model will never touch the relevance labels during the training process</i></b><br/><br/><br/>\n",
        "</p> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "inUAfc6TQMVJ",
        "colab": {},
        "outputId": "1c351b9a-0314-4c76-a4bf-f4285e4b5df8"
      },
      "source": [
        "! pip install git+https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git@master\n",
        "! pip install nltk\n",
        "! pip install rank_bm25"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git@master\n",
            "  Cloning https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git (to revision master) to /tmp/pip-req-build-8hrgjamc\n",
            "  Running command git clone -q https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git /tmp/pip-req-build-8hrgjamc\n",
            "Requirement already satisfied (use --upgrade to upgrade): pv211-utils==0.1.dev22+g3e6e680 from git+https://gitlab.fi.muni.cz/xstefan3/pv211-utils.git@master in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages\n",
            "Requirement already satisfied: setuptools in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from pv211-utils==0.1.dev22+g3e6e680) (46.1.3.post20200330)\n",
            "Requirement already satisfied: gspread in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from pv211-utils==0.1.dev22+g3e6e680) (3.6.0)\n",
            "Requirement already satisfied: oauth2client in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from pv211-utils==0.1.dev22+g3e6e680) (4.1.3)\n",
            "Requirement already satisfied: google in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from pv211-utils==0.1.dev22+g3e6e680) (2.0.3)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from gspread->pv211-utils==0.1.dev22+g3e6e680) (0.4.1)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from gspread->pv211-utils==0.1.dev22+g3e6e680) (1.13.1)\n",
            "Requirement already satisfied: requests>=2.2.1 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from gspread->pv211-utils==0.1.dev22+g3e6e680) (2.23.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (4.0)\n",
            "Requirement already satisfied: six>=1.6.1 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (1.14.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (0.17.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from oauth2client->pv211-utils==0.1.dev22+g3e6e680) (0.2.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from google->pv211-utils==0.1.dev22+g3e6e680) (4.9.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from google-auth-oauthlib>=0.4.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from google-auth>=1.12.0->gspread->pv211-utils==0.1.dev22+g3e6e680) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from requests>=2.2.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (1.25.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from requests>=2.2.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from requests>=2.2.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from requests>=2.2.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (2.9)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from beautifulsoup4->google->pv211-utils==0.1.dev22+g3e6e680) (2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread->pv211-utils==0.1.dev22+g3e6e680) (3.1.0)\n",
            "Building wheels for collected packages: pv211-utils\n",
            "  Building wheel for pv211-utils (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pv211-utils: filename=pv211_utils-0.1.dev22+g3e6e680-py3-none-any.whl size=503621 sha256=47eeace200540481ecbda8fa25462d6e5adeeae9a18e079e2276d3c3236f9508\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y09ghv9a/wheels/dc/9b/d5/10a4ccaca70692ea642f09d569959afdeec904f5cc91694ebc\n",
            "Successfully built pv211-utils\n",
            "Requirement already satisfied: nltk in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (3.5)\n",
            "Requirement already satisfied: tqdm in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from nltk) (4.45.0)\n",
            "Requirement already satisfied: joblib in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from nltk) (0.14.1)\n",
            "Requirement already satisfied: regex in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from nltk) (2020.4.4)\n",
            "Requirement already satisfied: click in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from nltk) (7.1.1)\n",
            "Requirement already satisfied: rank_bm25 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (0.2)\n",
            "Requirement already satisfied: numpy in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from rank_bm25) (1.18.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fyAqWIQyINng",
        "colab": {},
        "outputId": "0c733ec5-220b-440a-8468-72147b5ed309"
      },
      "source": [
        "from pv211_utils.entities import DocumentBase\n",
        "from pv211_utils.entities import QueryBase\n",
        "from pv211_utils.irsystem import IRSystem\n",
        "from pv211_utils.loader import load_documents\n",
        "from pv211_utils.loader import load_queries\n",
        "from pv211_utils.loader import load_judgements\n",
        "from pv211_utils.eval import mean_average_precision\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from rank_bm25 import BM25Plus\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from random import shuffle\n",
        "from collections import OrderedDict\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class Document(DocumentBase):       \n",
        "    def __init__(self, document_id, authors, bibliography, title, body):\n",
        "        super().__init__(document_id, authors, bibliography, title, body)\n",
        "        self.preprocessed = preprocess_text(self.body)\n",
        "\n",
        "        \n",
        "class Query(QueryBase):\n",
        "    def __init__(self, query_id, body):\n",
        "        super().__init__(query_id, body)\n",
        "        self.preprocessed = preprocess_text(self.body)\n",
        "\n",
        "\n",
        "sm_st = SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words('english')) | \\\n",
        "             set(['', 'co', 'eq', 'pr', 'tr', 'rl', 'psf', 'ko', 'la', 'vz', 'plk', 'o', 'etc', 'igy', 'soc',\n",
        "                   'ic', 'ible','ser', 'ing', 'ob', 'feb', 'wkb', 'ao', 'dp', 'tne', 'sr', 'ux', 'som', 'aft', 'con',\n",
        "                   'rev', 'j', 'b', 'p', 'e', 'a', 'sq', 'op', 'er', 'oc', 'ab', 'bc', 'de', 'im', 'fs', 'vs', 'rf',\n",
        "                   'bi','et', 'al', 'th', 'rd', 'nd', 'pb', 'rt', 'rm', 'qn', 'fd', 'qe', 'qm', 'de', 'vas', 'fig',\n",
        "                   'ty', 'tx', 'tz', 'pai', 'ied', 'ref', 'thn', 'jan', 'pre', 'mth', 'nth', 'uhf', 'btu', 'ink', 'rae',\n",
        "                   'ofr', 'n','f', 'dx', 'dy', 'dz', 'x', 'xx', 'y', 'yy', 'z', 'zz', 'q', 's', 'ax', 'cx', 'cf', 'b',\n",
        "                   'du','u', 'r', 'h', 'l', 'jmin','jmjn','viz', 'fl', 'ld', 'dvl', 're', 'tn', 'aec', 'k', 'i', 'ii',\n",
        "                   'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x', 'xi', 'xii', 'xiii', 'xiv', 'xv', 'xvi', 'xvii',\n",
        "                   'xviii', 'xix', 'xx']) \n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    sentences = re.split(r'\\s\\.\\s', text)\n",
        "    prepr_sentences = []\n",
        "    for s in sentences:\n",
        "        word_tokens = list(filter(None, re.split('[\\s,\\(\\)0-9\\'\\:\\$\\*\\;\\?\\\"\\/\\+\\=\\-]|[\\s.*\\.]|(aero|air|aer|super|sub|hyper|ultra|sonic|retro|poly|multi|therm|magnet|hydro|fero|accel|electro|photo|strobo|alumin|anti|less|non|post|cross|eigen|dynam|curv|cylind|linear|off|ellip|compress|molecul|metal|correl|stream|visc|crystall)', s)))\n",
        "        prepr_sentence_tokens = [sm_st.stem(w) for w in word_tokens if not w in stop_words]\n",
        "        if len(prepr_sentence_tokens) != 0:\n",
        "            prepr_sentences.append(prepr_sentence_tokens)\n",
        "    return prepr_sentences\n",
        "\n",
        "\n",
        "def find_max_length(texts):\n",
        "    l=[]\n",
        "    for text in texts.values():\n",
        "        l.append(len(set(flatten_text(text.preprocessed))))\n",
        "    return max(l), np.argmax(l)\n",
        "\n",
        "\n",
        "def flatten_text(text):\n",
        "    return [w for s in text for w in s]\n",
        "        \n",
        "    \n",
        "def create_new_doc_odict():\n",
        "    new_doc_odict = OrderedDict()\n",
        "    cnt = 0\n",
        "    for doc in documents.values():\n",
        "        if doc.body != '':\n",
        "            cnt += 1\n",
        "            new_doc_odict[cnt] = doc\n",
        "    return new_doc_odict\n",
        "    \n",
        "    \n",
        "def count_term_frequencies(texts):\n",
        "    words_fr = {}\n",
        "    for text in texts:\n",
        "        for sentence in text.preprocessed:\n",
        "            for w in sentence:\n",
        "                if w not in words_fr.keys():\n",
        "                    words_fr[w] = 1\n",
        "                else:\n",
        "                    words_fr[w] = words_fr[w] + 1\n",
        "    return words_fr\n",
        "                    \n",
        "\n",
        "def remove_rare_terms_from_words_fr_dict(words_fr):\n",
        "    words_fr_filtered = {}\n",
        "    for w, fr in words_fr.items():\n",
        "        if fr > 1:\n",
        "            words_fr_filtered[w] = fr\n",
        "    return words_fr_filtered\n",
        "\n",
        "\n",
        "def filter_text(text, words):\n",
        "    filtered_text = []\n",
        "    for s in text:\n",
        "        filtered_sentence = []\n",
        "        for w in s:\n",
        "            if w not in words:\n",
        "                filtered_sentence.append(w)\n",
        "        if len(filtered_sentence)>0:\n",
        "            filtered_text.append(filtered_sentence)\n",
        "    return filtered_text\n",
        "\n",
        "\n",
        "def create_words_to_ids_dict():\n",
        "    words_to_ids = {}\n",
        "    words_fr = count_term_frequencies(list(documents.values()) + list(queries.values()))\n",
        "    words_fr = remove_rare_terms_from_words_fr_dict(words_fr)\n",
        "    unique_terms = gather_unique_terms(words_fr)    \n",
        "    words_to_ids = {'CLS':0, 'SEP':1}    \n",
        "    cnt = 2\n",
        "    for w in sorted(unique_terms):\n",
        "        words_to_ids[w] = cnt\n",
        "        cnt+=1\n",
        "    return words_to_ids, sorted(list(unique_terms))\n",
        "\n",
        "\n",
        "def gather_unique_terms(words_fr):\n",
        "    unique_terms = gather_unique_terms_from_text(queries)\n",
        "    remove_duplicates(documents, unique_terms, words_fr)\n",
        "    remove_duplicates(queries, unique_terms, words_fr)\n",
        "    return set(unique_terms) & set(words_fr.keys())\n",
        "\n",
        "\n",
        "def gather_unique_terms_from_text(texts):\n",
        "    unique_terms = []\n",
        "    for t in texts.values():\n",
        "        for s in t.preprocessed:\n",
        "            for w in s:\n",
        "                if w not in unique_terms:\n",
        "                    unique_terms.append(w)\n",
        "    return unique_terms\n",
        "\n",
        "\n",
        "def remove_duplicates(texts, unique_terms, words_fr):\n",
        "    for t in texts.values():\n",
        "        filtered_text = []\n",
        "        for s in t.preprocessed:\n",
        "            filtered_sentence = []\n",
        "            for w in s:\n",
        "                if w in unique_terms and w in words_fr.keys():\n",
        "                    filtered_sentence.append(w)\n",
        "            if len(filtered_sentence)>0:\n",
        "                filtered_text.append(filtered_sentence)\n",
        "        t.preprocessed = filtered_text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/kolja/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/kolja/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBBjJiQ0Sb7a",
        "colab_type": "code",
        "colab": {},
        "outputId": "12098b4b-3cb2-4c6f-e2b1-54d69b2026e4"
      },
      "source": [
        "documents = load_documents(Document)\n",
        "queries = load_queries(Query)\n",
        "relevant = load_judgements(queries, documents)\n",
        "\n",
        "documents = create_new_doc_odict()\n",
        "words_to_ids, unique_terms = create_words_to_ids_dict()\n",
        "vocab_size = len(words_to_ids.keys())\n",
        "num_docs = len(documents.values())\n",
        "num_queries = len(queries.values())\n",
        "\n",
        "print('Documents: ', num_docs)\n",
        "print('Queries: ', num_queries)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documents:  1398\n",
            "Queries:  225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25jYyse3Sb7f",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<h3 align=\"center\" style=\"text-align:center;\"><i>First-stage retrieval algorithm</i><br/><br></h3>\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/><i>Preprosessing</i> pipeline includes following steps:<br/><br/></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li>Text is splitted into sentences, sentences are splitted into the tokens, <i>stopwords</i> are removed and each token is <i>stemmed</i> by <i>SnowballStemmer</i><br/><br/></li>\n",
        "<li><i>Relevance</i> scores between <i>query</i> and <i>each document</i> are calculated using <i>BM25Plus</i> algorithm with <i>default</i> configuration (no explicit optimization on hyperparameters). The most <i>relevant documents</i> appear at the top of retrieved list of documents. Howewer, the <i>order</i> of documents may be <i>inacurate.</i></li></ul><br>\n",
        "\n",
        "\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ2vlEaqSb7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BM25IRSystem(IRSystem):\n",
        "    def __init__(self):\n",
        "        self.documents = list(documents.values())\n",
        "        self.corpus = [flatten_text(doc.preprocessed) for doc in self.documents]\n",
        "        self.bm25 = BM25Plus(self.corpus)\n",
        "\n",
        "        \n",
        "    def first_stage_ranking(self, preprocessed_query):\n",
        "        scores = self.bm25.get_scores(preprocessed_query.preprocessed[0])\n",
        "        doc_list_ids = np.argsort(scores)[::-1]\n",
        "        return np.take(self.documents, doc_list_ids, axis=0).tolist()[:]\n",
        "\n",
        "                \n",
        "    def search(self, query):\n",
        "        preprocessed_query = queries[query.query_id]\n",
        "        fs_ranked_docs = self.first_stage_ranking(preprocessed_query)\n",
        "        return fs_ranked_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1OzfxtUSb7n",
        "colab_type": "code",
        "colab": {},
        "outputId": "17885fd8-1f19-49b7-9416-c14c2c95eacc"
      },
      "source": [
        "mean_average_precision(BM25IRSystem(), submit_result=False, author_name=\"Kaplenko, Mykola\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean average precision: 42.703% \n",
            "Not submitted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMpukwRkSb7w",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<h3 align=\"center\" style=\"text-align:center;\"><i>Now, let's train the second-stage reranking model: \"tiny\" BERT</i><br/><br></h3>\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/>    \n",
        "Obviously, reranking models are trained in a <i>supervised</i> manner using the <i>relevance labels</i>: model takes <i>2</i> documents as inputs (one document is <i>relevant</i>, another document is always <i>non-relevant</i>), model then tries to guess which of <i>2</i> input documents is <i>relevant</i>.<br/><br/></p>\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/>  \n",
        "We <b>will not use</b> the <i>relevance labels</i>. Instead, we slightly reformulate our task in order to train the model without any supervision from the <i>relevance labels</i>.<br/><br/></p>\n",
        "\n",
        "<p style=\"font-size:120%;\"> The <i>key idea</i> is to train our model in a such a way that it will develop the <i>general skill</i> of languge understanding.<br/><br/><br/>\n",
        "Training process is organized as follows:<br/><br/></p>\n",
        "<ul style=\"font-size:120%;\"><li>\n",
        "We randomly choose <i>2</i> documents from the list of documents. Randomly extract several (let's say <i>N</i>) words from the <i>first document</i> and randomly remove <i>K</i> words from the <i>second document</i> such that the documents now have the <i>same</i> length<br/><br/></li>\n",
        "<li>Model takes <i>3</i> inputs: the <i>extracted words</i>, the <i>first document</i> (without <i>extracted words</i>) and the <i>second document</i> (from which K words were removed)</li>\n",
        "<br/>\n",
        "<li>Model tries to answer the <i>question</i>: do <i>extracted words</i> belong to the <i>first document</i> or to the <i>second one</i>? We know from which one document we <i>extracted</i> the words, so we can train the model in the <i>supervised</i> manner, but we <b>do not use</b> the <i>relevance labels</i><br/><br/></li></ul>\n",
        "\n",
        "<p style=\"font-size:120%;\">\n",
        "To <i>succesfully</i> perform its task, the model has to learn the <i>relations</i> between the <i>context</i> and the <i>words</i>, whose <i>meaning</i> depends on this <i>context</i>.<br/><br/></p>\n",
        "\n",
        "<img src=\"https://github.com/xkaple01/bert-information-retrieval/blob/master/bert_ir_system_gc/multiple_choice_2.png?raw=1\" align=\"middle\">\n",
        "\n",
        "<br>\n",
        "\n",
        "<p style=\"font-size:120%;\"><br/>\n",
        "But <i><b>how</b></i> can we use this to rerank documents?<br/><br/></p>\n",
        "\n",
        "<p style=\"font-size:120%;\">\n",
        "Trained model again takes <i>3</i> inputs: <i>query</i>, <i>whole firs document</i> (with no <i>words extracted</i>), and <i>whole second document</i>. Model then <i>predicts</i>: is it more likely that the <i>query words</i> originate (\"<i>were extracted</i>\") from the <i>first document</i> or from the <i>second document</i>?<br/><br/><br/> Documents in the list of retrieved documents are then <i>pairwise</i> rearanged in such a way, that the more relevant documents occur in the list at higher positions than the less relevant documents.<br/><br></p>\n",
        "<hr>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scxTTGFLSb7x",
        "colab_type": "text"
      },
      "source": [
        "<p style=\"font-size:120%;\">\n",
        "To <i>proceed</i> to the next section, we will need the <i>Tensorflow-2</i> being installed. The <i>best</i> option is to have the separate <i>Anaconda environment</i> with <i>Tensorflow</i> installed inside it.<br/><br/><br/>\n",
        "So, activate your <i>environment</i> (if you <i>already</i> have one) or type the following commands in your <i>shell</i> to create the <i>new enironment</i>:<br/><br/></p>\n",
        "\n",
        "```\n",
        "conda create -n tensorflow-2-gpu tensorflow-gpu\n",
        "conda activate tensorflow-2-gpu\n",
        "```\n",
        "\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlgND5xzSb7y",
        "colab_type": "code",
        "colab": {},
        "outputId": "0e992390-9b5d-4748-9fc4-d667e7b94d5b"
      },
      "source": [
        "import tensorflow as tf\n",
        "! pip install transformers\n",
        "from transformers import *\n",
        "\n",
        "PATH_TO_LOGS = './logs'\n",
        "PATH_TO_MODEL = './model/model_checkpoint' \n",
        "\n",
        "def create_directory(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (2.8.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from transformers) (2020.4.4)\n",
            "Requirement already satisfied: numpy in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from transformers) (4.45.0)\n",
            "Requirement already satisfied: sentencepiece in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: sacremoses in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: filelock in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from transformers) (1.13.0)\n",
            "Requirement already satisfied: click in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
            "Requirement already satisfied: joblib in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.0 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from boto3->transformers) (1.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from botocore<1.17.0,>=1.16.0->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /home/kolja/anaconda3/envs/tensorflow-2-gpu/lib/python3.7/site-packages (from botocore<1.17.0,>=1.16.0->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6LJis47Sb79",
        "colab_type": "code",
        "colab": {},
        "outputId": "a95310a7-8dda-4790-b721-13390a6ab698"
      },
      "source": [
        "create_directory(PATH_TO_LOGS)\n",
        "create_directory(PATH_TO_MODEL)\n",
        "\n",
        "documents = load_documents(Document)\n",
        "queries = load_queries(Query)\n",
        "relevant = load_judgements(queries, documents)\n",
        "\n",
        "documents = create_new_doc_odict()\n",
        "words_to_ids, unique_terms = create_words_to_ids_dict()\n",
        "vocab_size = len(words_to_ids.keys())\n",
        "num_docs = len(documents.values())\n",
        "num_queries = len(queries.values())\n",
        "\n",
        "print('Documents: ', num_docs)\n",
        "print('Queries: ', num_queries)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Documents:  1398\n",
            "Queries:  225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRY7z0_dSb8E",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<h3 align=\"center\"><i>Let's create the data generators:</i><br/><br></h3>\n",
        "<p style=\"font-size:120%;\">Note, the <i>TrainDataGenerator</i> never touches the relevance pairs (query - relevant document) for the training.\n",
        "Relevance labels are only used in <i>ValidationDataGenerator</i> to monitor the model performance during the training and <b>does not affect</b> the training process.</p>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmUlugcpSb8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainDataGenerator(tf.keras.utils.Sequence):   \n",
        "    def __len__(self):\n",
        "        return 300\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_x_input_ids = []\n",
        "        data_x_attention_masks = []\n",
        "        data_x_token_type_ids = []\n",
        "        data_x_position_ids = []\n",
        "        data_y = []\n",
        "        for _ in range(BATCH_SIZE):\n",
        "            while True:                  \n",
        "                random_doc_ids = np.random.choice(num_docs, 2 , replace=False) + 1\n",
        "                random_docs = np.array([documents[random_doc_ids[0]], documents[random_doc_ids[1]]])\n",
        "                if not bool(set(random_docs[0].preprocessed[0]) & set(random_docs[1].preprocessed[0])):\n",
        "                    break\n",
        "                                \n",
        "            polarity = np.random.choice(2, 2, replace=False)\n",
        "            p_doc = random_docs[0]\n",
        "            n_doc = random_docs[1]\n",
        "            \n",
        "            p_random_words, p_words, n_words = select_random_words_from_texts(p_doc.preprocessed, n_doc.preprocessed)\n",
        "           \n",
        "            text1 = p_random_words\n",
        "            p_text = p_words\n",
        "            n_text = n_words\n",
        "\n",
        "            n_p_texts = np.array([n_text, p_text])\n",
        "            random_texts = np.take(n_p_texts, polarity, axis=0).tolist()\n",
        "            text2 = random_texts[0]\n",
        "            text3 = random_texts[1]\n",
        "\n",
        "            sample_input_ids, sample_attention_mask, sample_token_type_ids, sample_position_ids = create_sample(text1, text2, text3)\n",
        "            label = polarity\n",
        "            \n",
        "            data_x_input_ids.append(sample_input_ids)\n",
        "            data_x_attention_masks.append(sample_attention_mask)\n",
        "            data_x_token_type_ids.append(sample_token_type_ids)\n",
        "            data_x_position_ids.append(sample_position_ids)\n",
        "            data_y.append(label)\n",
        "            \n",
        "        return ([np.array(data_x_input_ids), np.array(data_x_attention_masks), np.array(data_x_token_type_ids), np.array(data_x_position_ids)], np.array(data_y))\n",
        "\n",
        "\n",
        "    \n",
        "class ValidationDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self):\n",
        "        self.rel_pairs = list(relevant)\n",
        "        self.num_rel_pairs = len(self.rel_pairs)\n",
        "        self.len = 25\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_x_input_ids = []\n",
        "        data_x_attention_masks = []\n",
        "        data_x_token_type_ids = []\n",
        "        data_x_sample_position_ids = []\n",
        "        data_y = []\n",
        "        for _ in range(BATCH_SIZE):\n",
        "            random_pair_idx = np.random.randint(self.num_rel_pairs, size=1).item()\n",
        "            random_pair = self.rel_pairs[random_pair_idx]\n",
        "            polarity = np.random.choice(2, 2, replace=False)\n",
        "              \n",
        "            while True:\n",
        "                n_random_doc_id = np.random.randint(num_docs, size=1).item() + 1\n",
        "                if not (random_pair[0], documents[n_random_doc_id]) in relevant:\n",
        "                    break\n",
        "\n",
        "            p_text = random_pair[1].preprocessed  \n",
        "            n_text = documents[n_random_doc_id].preprocessed\n",
        "        \n",
        "            p_random_words, p_words, n_words = select_random_words_from_texts(p_text, n_text)\n",
        "           \n",
        "            text1 = p_random_words\n",
        "            p_text = p_words\n",
        "            n_text = n_words \n",
        "        \n",
        "            n_p_texts = np.array([n_text, p_text])\n",
        "            random_texts = np.take(n_p_texts, polarity, axis=0).tolist()\n",
        "            text2 = random_texts[0]\n",
        "            text3 = random_texts[1]\n",
        "            \n",
        "            sample_input_ids, sample_attention_mask, sample_token_type_ids, sample_position_ids = create_sample(text1, text2, text3)\n",
        "            label = polarity\n",
        "            \n",
        "            data_x_input_ids.append(sample_input_ids)\n",
        "            data_x_attention_masks.append(sample_attention_mask)\n",
        "            data_x_token_type_ids.append(sample_token_type_ids)\n",
        "            data_x_sample_position_ids.append(sample_position_ids)\n",
        "            data_y.append(label)\n",
        "            \n",
        "        return ([np.array(data_x_input_ids), np.array(data_x_attention_masks), np.array(data_x_token_type_ids), np.array(data_x_sample_position_ids)], np.array(data_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehZXAWu1Sb8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_sample_part(text, token_type_id, position_id):\n",
        "    set(flatten_text(text))\n",
        "    words = []\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    token_type_ids = []\n",
        "    position_ids = []\n",
        "    for sentence in text:\n",
        "        for w in sentence:\n",
        "            if w in words_to_ids.keys():\n",
        "                words.append(w)\n",
        "                input_ids.append(words_to_ids[w])\n",
        "                attention_mask.append(1)\n",
        "                token_type_ids.append(token_type_id)\n",
        "                position_ids.append(position_id)\n",
        "    words.append('SEP')\n",
        "    input_ids.append(words_to_ids['SEP'])\n",
        "    attention_mask.append(1)\n",
        "    token_type_ids.append(token_type_id)\n",
        "    position_ids.append(position_id)\n",
        "    return words, input_ids, attention_mask, token_type_ids, position_ids\n",
        "\n",
        "\n",
        "def create_sample(text1, text2, text3):\n",
        "    sample_words = ['CLS']\n",
        "    sample_input_ids = [words_to_ids['CLS']]\n",
        "    sample_attention_mask = [1]\n",
        "    sample_token_type_ids = [0]\n",
        "    sample_position_ids = [0]\n",
        "\n",
        "    part1 = create_sample_part(text1, token_type_id=0, position_id=0)\n",
        "    part2 = create_sample_part(text2, token_type_id=1, position_id=0)\n",
        "    part3 = create_sample_part(text3, token_type_id=2, position_id=0)\n",
        "    \n",
        "    n_pad_1 = BERT_MAX_SEQ_LEN - (len(part1[0]) + len(part2[0]) + 1)\n",
        "    n_pad_2 = BERT_MAX_SEQ_LEN - (len(part1[0]) + len(part3[0]) + 1)\n",
        "\n",
        "    zero_pad_1 = np.zeros(n_pad_1, dtype='int32')\n",
        "    zero_pad_2 = np.zeros(n_pad_2, dtype='int32')\n",
        "    \n",
        "    sample_words_1 = np.concatenate([np.array(sample_words + part1[0] + part2[0]), zero_pad_1])\n",
        "    sample_input_ids_1 = np.concatenate([np.array(sample_input_ids + part1[1] + part2[1]), zero_pad_1])\n",
        "    sample_attention_mask_1 = np.concatenate([np.array(sample_attention_mask + part1[2] + part2[2]), zero_pad_1])\n",
        "    sample_token_type_ids_1 = np.concatenate([np.array(sample_token_type_ids + part1[3] + part2[3]), zero_pad_1])\n",
        "    sample_position_ids_1 = np.concatenate([np.array(sample_position_ids + part1[4] + part2[4]), zero_pad_1])\n",
        "    \n",
        "    sample_words_2 = np.concatenate([np.array(sample_words + part1[0] + part3[0]), zero_pad_2])\n",
        "    sample_input_ids_2 = np.concatenate([np.array(sample_input_ids + part1[1] + part3[1]), zero_pad_2])\n",
        "    sample_attention_mask_2 = np.concatenate([np.array(sample_attention_mask + part1[2] + part3[2]), zero_pad_2])\n",
        "    sample_token_type_ids_2 = np.concatenate([np.array(sample_token_type_ids + part1[3] + part3[3]), zero_pad_2])\n",
        "    sample_position_ids_2 = np.concatenate([np.array(sample_position_ids + part1[4] + part3[4]), zero_pad_2])\n",
        "    \n",
        "    sample_words = np.stack([sample_words_1, sample_words_2])\n",
        "    sample_input_ids = np.stack([sample_input_ids_1, sample_input_ids_2])\n",
        "    sample_attention_mask = np.stack([sample_attention_mask_1, sample_attention_mask_2])\n",
        "    sample_token_type_ids = np.stack([sample_token_type_ids_1, sample_token_type_ids_2])\n",
        "    sample_position_ids = np.stack([sample_position_ids_1, sample_position_ids_2])\n",
        "        \n",
        "    return sample_input_ids, sample_attention_mask, sample_token_type_ids, sample_position_ids\n",
        "\n",
        "\n",
        "def select_random_words_from_texts(p_text, n_text):\n",
        "    num_words_to_select = np.random.choice(range(MIN_NUM_WORDS_TO_SELECT, MAX_NUM_WORDS_TO_SELECT), 1).item()\n",
        "    p_words = flatten_text(p_text)\n",
        "    n_words = flatten_text(n_text)\n",
        "    \n",
        "    p_n_words = list(set(p_words) & set(n_words))\n",
        "    p_unique_words = list(set(p_words) - set(p_n_words))\n",
        "    n_unique_words = list(set(n_words) - set(p_n_words))\n",
        "    num_words_to_select = np.min([num_words_to_select, len(p_unique_words)//2, len(n_unique_words)//2])\n",
        "\n",
        "    shuffle(p_unique_words)\n",
        "    p_random_words = p_unique_words[:num_words_to_select]\n",
        "    \n",
        "    shuffle(n_unique_words)\n",
        "    n_random_words = n_unique_words[:num_words_to_select]\n",
        "\n",
        "    p_filtered_text = filter_text(p_text, p_random_words)\n",
        "    p_filtered_text_fl = flatten_text(p_filtered_text)\n",
        "    shuffle(p_filtered_text_fl)\n",
        "    p_words = p_filtered_text_fl[:num_words_to_select]\n",
        "      \n",
        "    return [p_random_words], [p_words], [n_random_words]\n",
        "\n",
        "\n",
        "\n",
        "def prepare_doc_batch(query, docs):\n",
        "    data_x_input_ids = []\n",
        "    data_x_attention_masks = []\n",
        "    data_x_token_type_ids = []\n",
        "    data_x_sample_position_ids = []\n",
        "    \n",
        "    for i in range(0, len(docs)-1, 2):             \n",
        "        p_words = flatten_text(docs[i].preprocessed)\n",
        "        n_words = flatten_text(docs[i+1].preprocessed)\n",
        "    \n",
        "        p_n_words = list(set(p_words) & set(n_words))\n",
        "        p_unique_words = list(set(p_words) - set(p_n_words))\n",
        "        n_unique_words = list(set(n_words) - set(p_n_words))\n",
        "          \n",
        "        text1 = query.preprocessed\n",
        "        text2 = [p_unique_words]\n",
        "        text3 = [n_unique_words]\n",
        "        \n",
        "        sample_input_ids, sample_attention_mask, sample_token_type_ids, sample_position_ids = create_sample(text1, text2, text3)\n",
        "        \n",
        "        data_x_input_ids.append(sample_input_ids)\n",
        "        data_x_attention_masks.append(sample_attention_mask)\n",
        "        data_x_token_type_ids.append(sample_token_type_ids)\n",
        "        data_x_sample_position_ids.append(sample_position_ids)\n",
        "        \n",
        "    return [np.array(data_x_input_ids), np.array(data_x_attention_masks), np.array(data_x_token_type_ids), np.array(data_x_sample_position_ids)]\n",
        "      \n",
        "    \n",
        "    \n",
        "def swap_docs_based_on_predictions(raw_doc_batch, predictions):\n",
        "    reranked_docs = raw_doc_batch   \n",
        "    for i in range(predictions.shape[0]):\n",
        "        if predictions[i][0] + 0.2 < predictions[i][1]:\n",
        "            t = reranked_docs[2*i]\n",
        "            reranked_docs[2*i] = reranked_docs[2*i+1]\n",
        "            reranked_docs[2*i+1] = t            \n",
        "    return reranked_docs\n",
        "        \n",
        "    \n",
        "    \n",
        "def rerank_docs_based_on_query(query, fs_ranked_documents):   \n",
        "    docs_to_rerank = fs_ranked_documents[4:44]\n",
        "    prepared_batch = prepare_doc_batch(query, docs_to_rerank)\n",
        "    predictions = model.predict(prepared_batch, batch_size=BATCH_SIZE)\n",
        "    reranked_docs = swap_docs_based_on_predictions(docs_to_rerank, predictions)\n",
        "    fs_ranked_documents[4:44] = reranked_docs \n",
        "    return fs_ranked_documents  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQGusX74Sb8T",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<h3 align=\"center\"><i>Now, the most interesting part - we define the model:</i><br/><br></h3>\n",
        "<p style=\"font-size:120%;\">Model's inputs have the following structure: <br/></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li><i>input_ids</i>: each token has its unique <i>id</i>, this <i>id</i> is used in the <i>BERT</i> <i>embedding layer</i> to hash the integer index to the numeric vector representing the token as the float-pointing numbers</li><br/>\n",
        "<li><i>attention_mask</i>: model always takes the input of predefined length; if the documents are shorter than this predefined length - they are padded with zeros; attention mask then indicates which tokens belong to the documents and which tokens (padded zeros) were appended to match the required input length</li><br/>\n",
        "<li><i>token_type_ids</i>: indicate whether token correnspond to the first or to the second input document</li><br/>\n",
        "<li><i>position_ids</i>: encode the order of words in documents</li><br/></ul></p>\n",
        "\n",
        "<p style=\"font-size:120%;\">\n",
        "Model predicts <i>2</i> softmax scores: probability that the words were extracted from the first document and the probability that the words were extraceted from the second one<br><br></p>\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjI4fwc9Sb8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(bert_config):\n",
        "    num_choices = 2\n",
        "    bert = TFBertForMultipleChoice(bert_config)\n",
        "    input_ids = tf.keras.Input(shape=(num_choices, BERT_MAX_SEQ_LEN,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = tf.keras.Input(shape=(num_choices, BERT_MAX_SEQ_LEN,), dtype=tf.int32, name='attention_mask')\n",
        "    token_type_ids = tf.keras.Input(shape=(num_choices, BERT_MAX_SEQ_LEN,), dtype=tf.int32, name='token_type_ids')\n",
        "    position_ids = tf.keras.Input(shape=(num_choices, BERT_MAX_SEQ_LEN,), dtype=tf.int32, name='position_ids')\n",
        "    \n",
        "    \n",
        "    logits = bert([input_ids, attention_mask, token_type_ids, position_ids])[0]\n",
        "    output = tf.keras.layers.Softmax(input_shape=(num_choices,))(logits)\n",
        "    model = tf.keras.Model([input_ids, attention_mask, token_type_ids, position_ids], output)\n",
        "    \n",
        "    \n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksN1wWM_Sb8c",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<h3 align=\"center\"><i>Configure the model for training:<br/><br></i></h3>\n",
        "<p style=\"font-size:120%;\">Note, that we train the model from <i>scratch</i>. Training process may be unstable, so:</p>\n",
        "<ul style=\"font-size:120%;\"> \n",
        "<li>Use the batch size at least <i>128</i> (if the batch <i>does not</i> fit to memory - you will get the <i>OOM error</i> - just <i>decrease</i> the batch size)</li><br/>\n",
        "<li>Use the <i>lower</i> learning rate (<i>i.e. 3e-6</i>)</li><br/><br/></ul>\n",
        "\n",
        "\n",
        "<p style=\"font-size:120%;\">You can experiment with different settings: <br/></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li>Increase the batch size $n$ times means the reduction of <i>gradient variance</i> by factor $\\sqrt{n}$. In other words, we <i>do not</i> benefit from the <i>too high</i> batch sizes - they still <i>do not</i> provide the true estimation of <i>gradient</i>, but significantly <i>slow down</i> the training. On the other hand, with the <i>too small</i> batch size the model can simply <i>not to converge</i> at all</li><br/>\n",
        "<li>If you decrese the batch size $n$ times, you have to decrese the learning rate by factor $\\sqrt{n}$ to preserve the stability of learning process</li><br/>\n",
        "<li><i>Epochs</i> is just an arbitrary high number. Model weights are saved after <i>each</i> epoch through the <i>ModelCheckpoint</i> callback, so feel free to stop the learning whenever you want (when the model will be <i>precise</i> enough)<br><br><br></li>\n",
        "    \n",
        "    \n",
        "<p style=\"font-size:120%;\">Model whose performance is reported below was trained with the <i>initial</i> learning rate <i>1e-5</i> to achieve the faster convergence at the <i>initial</i> phase of training. Then, the learning rate was decreased to <i>3e-6</i> and the network continued to train during the <i>14</i> additional hours. The whole training process took approximately <i>18</i> hours (on the <i>weeny</i> but <i>proud</i> <i>GeForce GTX 850M</i> GPU). Model has only <i>65k</i> parameters. Experiments with <i>higher learning rates</i> led to the model <i>divergence</i>; <i>smaller batch sizes</i> (<i>i.e. 8</i>) - <i>convergence</i> to the <i>local minimums</i>, <i>performans degradation</i> and the <i>disability</i> to return to the normal training process.<br><br><br></p>\n",
        "    \n",
        "    \n",
        "<p style=\"font-size:120%;\">Model performance in the final phase of training: <br><br></p>\n",
        "\n",
        "<img src=\"https://github.com/xkaple01/bert-information-retrieval/blob/master/bert_ir_system_gc/learning_process.gif?raw=1\" align=\"middle\">\n",
        "<br><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb7vRJQUSb8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.000003\n",
        "EPOCHS = 1000\n",
        "MIN_NUM_WORDS_TO_SELECT = 75\n",
        "MAX_NUM_WORDS_TO_SELECT = 76 \n",
        "BERT_MAX_SEQ_LEN = 25 + MAX_NUM_WORDS_TO_SELECT + 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twiLYulcSb8k",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<h3><i>Original BERT:</i><br/><br></h3>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li><i>12</i> attention layers</li><br/>\n",
        "<li>Each token (english has approximately <i>30000</i> of the most commonly used words) is encoded as a vecor of <i>512</i> numbers</li><br/>\n",
        "<li>Memory size (dimensionality of pointwise dense layers in encoder blocks) is <i>3072</i></li></ul><br/>\n",
        "<p style=\"font-size:120%;\">As a result, the model requires the <i>gigabytes</i> of textual data, <i>64</i> GPUs and a <i>week</i> of training. <br/><br/> We will design the <i>\"tiny\"</i> version of <i>BERT</i>. The model has to be powerful enough to capture all the hidden relations between words and at the same time has not to be overparametrized in order not to overfit hardly.</p><br/><br/>\n",
        "    \n",
        "<p style=\"font-size:120%;\">Recommended settings are following:<br/><br/></p>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li><i>2</i> (but better at least <i>3</i> attention layers - this, ofcourse, will slow down the training and increase the inference time)</li><br/>\n",
        "<li>Hidden size is <i>32</i>: each token in our scientific Cranfield language will be represented as a vector of <i>32</i> numbers</li><br/>\n",
        "<li>Intermediate (memory size) is at least <i>256</i>: we want to reliably capture the hidden relations between words</li><br/>\n",
        "<li>Number of attention heads is just <i>1</i>. The dot products between such a small (<i>32</i>-dim) representations do not require the vector to be splitted into several parts and to be attended by multiple heads. But feel free to set the number of attention heads to <i>2</i> or <i>4</i></li>\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5fglg8-Sb8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_train_config = BertConfig(vocab_size=680, hidden_size=32, num_hidden_layers=2, num_attention_heads=1, intermediate_size=256, max_position_embeddings=3, type_vocab_size=3, training=True)\n",
        "\n",
        "train_data_generator = TrainDataGenerator()\n",
        "val_data_generator = ValidationDataGenerator()\n",
        "\n",
        "model = create_model(bert_train_config)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=PATH_TO_LOGS, histogram_freq=1, write_graph=True, write_images=False)\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=PATH_TO_MODEL, monitor='accuracy', verbose=1, save_best_only=False, save_weights_only=True, save_freq='epoch')\n",
        "\n",
        "\n",
        "history = model.fit(x=train_data_generator, validation_data = val_data_generator, epochs=EPOCHS, callbacks=[tb_callback, ckpt_callback])\n",
        "model.save_weights(PATH_TO_MODEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehj-4AELSb8t",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<p style=\"font-size:120%;\">Note how the <i>validation loss</i> is <i>improved</i> as the training proceeds. The increasing <i>language understanding general ability</i> of the model leads to improvement in the reranking accuracy even though the model <b>does not have</b> any information about the <i>query - document</i> relevance.</p><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh3qp4DmSb8u",
        "colab_type": "text"
      },
      "source": [
        "<h3 align=\"center\"><i>It's time to test our model:</i><br/><br></h3>\n",
        "<p style=\"font-size:120%;\">Let's configure the inference phase:</p><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQG2_LSCSb8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 1\n",
        "BERT_MAX_SEQ_LEN = 512 \n",
        "bert_inference_config = BertConfig(vocab_size=680, hidden_size=32, num_hidden_layers=2, num_attention_heads=1, intermediate_size=256, max_position_embeddings=3, type_vocab_size=3, training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr5mlHAJSb84",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<p style=\"font-size:120%;\">Alternatively, you can load the trained model in order to reproduce the final achieved result reported below</p><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04vgawW8Sb86",
        "colab_type": "code",
        "colab": {},
        "outputId": "dbcd8153-6680-453d-80d0-4e50c8785fb3"
      },
      "source": [
        "PATH_TO_MODEL  = './trained_model/model_checkpoint' \n",
        "model = create_model(bert_inference_config)\n",
        "model.load_weights(PATH_TO_MODEL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 2, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_mask (InputLayer)     [(None, 2, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 2, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "position_ids (InputLayer)       [(None, 2, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_multiple_choice (TF ((None, 2),)         65153       input_ids[0][0]                  \n",
            "                                                                 attention_mask[0][0]             \n",
            "                                                                 token_type_ids[0][0]             \n",
            "                                                                 position_ids[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Softmax)               (None, 2)            0           tf_bert_for_multiple_choice[0][0]\n",
            "==================================================================================================\n",
            "Total params: 65,153\n",
            "Trainable params: 65,153\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7efe30474410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p62UPjpSb9B",
        "colab_type": "code",
        "colab": {},
        "outputId": "e6232b56-5656-4540-d499-4f984961718b"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.000003\n",
        "EPOCHS = 1000\n",
        "MIN_NUM_WORDS_TO_SELECT = 75\n",
        "MAX_NUM_WORDS_TO_SELECT = 76 \n",
        "BERT_MAX_SEQ_LEN = 25 + MAX_NUM_WORDS_TO_SELECT + 4\n",
        "PATH_TO_MODEL  = './trained_model/model_checkpoint' \n",
        "bert_train_config = BertConfig(vocab_size=680, hidden_size=32, num_hidden_layers=2, num_attention_heads=1, intermediate_size=256, max_position_embeddings=3, type_vocab_size=3, training=True)\n",
        "\n",
        "train_data_generator = TrainDataGenerator()\n",
        "val_data_generator = ValidationDataGenerator()\n",
        "\n",
        "model = create_model(bert_train_config)\n",
        "model.load_weights(PATH_TO_MODEL)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=PATH_TO_LOGS, histogram_freq=1, write_graph=True, write_images=False)\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=PATH_TO_MODEL, monitor='accuracy', verbose=1, save_best_only=False, save_weights_only=True, save_freq='epoch')\n",
        "\n",
        "\n",
        "history = model.fit(x=train_data_generator, validation_data = val_data_generator, epochs=EPOCHS, callbacks=[tb_callback, ckpt_callback])\n",
        "model.save_weights(PATH_TO_MODEL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 2, 105)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_mask (InputLayer)     [(None, 2, 105)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 2, 105)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "position_ids (InputLayer)       [(None, 2, 105)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_multiple_choice (TF ((None, 2),)         65153       input_ids[0][0]                  \n",
            "                                                                 attention_mask[0][0]             \n",
            "                                                                 token_type_ids[0][0]             \n",
            "                                                                 position_ids[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Softmax)               (None, 2)            0           tf_bert_for_multiple_choice[0][0]\n",
            "==================================================================================================\n",
            "Total params: 65,153\n",
            "Trainable params: 65,153\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 300 steps, validate for 25 steps\n",
            "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
            "Epoch 1/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.9023\n",
            "Epoch 00001: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 68s 225ms/step - loss: 0.2380 - accuracy: 0.9023 - val_loss: 0.1800 - val_accuracy: 0.9278\n",
            "Epoch 2/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9024\n",
            "Epoch 00002: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2375 - accuracy: 0.9024 - val_loss: 0.1523 - val_accuracy: 0.9400\n",
            "Epoch 3/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.8999\n",
            "Epoch 00003: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 212ms/step - loss: 0.2396 - accuracy: 0.8999 - val_loss: 0.1738 - val_accuracy: 0.9331\n",
            "Epoch 4/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2314 - accuracy: 0.9044\n",
            "Epoch 00004: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.2316 - accuracy: 0.9042 - val_loss: 0.1670 - val_accuracy: 0.9325\n",
            "Epoch 5/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2324 - accuracy: 0.9044\n",
            "Epoch 00005: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2326 - accuracy: 0.9044 - val_loss: 0.1707 - val_accuracy: 0.9384\n",
            "Epoch 6/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2296 - accuracy: 0.9040\n",
            "Epoch 00006: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2296 - accuracy: 0.9039 - val_loss: 0.1710 - val_accuracy: 0.9353\n",
            "Epoch 7/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9038\n",
            "Epoch 00007: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2373 - accuracy: 0.9037 - val_loss: 0.1739 - val_accuracy: 0.9328\n",
            "Epoch 8/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2331 - accuracy: 0.9037\n",
            "Epoch 00008: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2331 - accuracy: 0.9038 - val_loss: 0.1588 - val_accuracy: 0.9409\n",
            "Epoch 9/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.9021\n",
            "Epoch 00009: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2350 - accuracy: 0.9021 - val_loss: 0.1688 - val_accuracy: 0.9325\n",
            "Epoch 10/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9015\n",
            "Epoch 00010: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2389 - accuracy: 0.9016 - val_loss: 0.1681 - val_accuracy: 0.9350\n",
            "Epoch 11/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2325 - accuracy: 0.9056\n",
            "Epoch 00011: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2328 - accuracy: 0.9054 - val_loss: 0.1823 - val_accuracy: 0.9225\n",
            "Epoch 12/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2338 - accuracy: 0.9031\n",
            "Epoch 00012: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2340 - accuracy: 0.9031 - val_loss: 0.1678 - val_accuracy: 0.9369\n",
            "Epoch 13/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2334 - accuracy: 0.9048\n",
            "Epoch 00013: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2334 - accuracy: 0.9047 - val_loss: 0.1577 - val_accuracy: 0.9438\n",
            "Epoch 14/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2299 - accuracy: 0.9040\n",
            "Epoch 00014: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2298 - accuracy: 0.9041 - val_loss: 0.1702 - val_accuracy: 0.9344\n",
            "Epoch 15/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2274 - accuracy: 0.9071\n",
            "Epoch 00015: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2276 - accuracy: 0.9070 - val_loss: 0.1721 - val_accuracy: 0.9300\n",
            "Epoch 16/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9058\n",
            "Epoch 00016: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2325 - accuracy: 0.9058 - val_loss: 0.1652 - val_accuracy: 0.9378\n",
            "Epoch 17/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2323 - accuracy: 0.9049\n",
            "Epoch 00017: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2324 - accuracy: 0.9050 - val_loss: 0.1622 - val_accuracy: 0.9372\n",
            "Epoch 18/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2318 - accuracy: 0.9041\n",
            "Epoch 00018: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2322 - accuracy: 0.9040 - val_loss: 0.1646 - val_accuracy: 0.9366\n",
            "Epoch 19/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9050\n",
            "Epoch 00019: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2313 - accuracy: 0.9050 - val_loss: 0.1678 - val_accuracy: 0.9347\n",
            "Epoch 20/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9046\n",
            "Epoch 00020: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2308 - accuracy: 0.9048 - val_loss: 0.1549 - val_accuracy: 0.9397\n",
            "Epoch 21/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "299/300 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9038\n",
            "Epoch 00021: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.2311 - accuracy: 0.9038 - val_loss: 0.1623 - val_accuracy: 0.9391\n",
            "Epoch 22/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.9070\n",
            "Epoch 00022: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2318 - accuracy: 0.9070 - val_loss: 0.1648 - val_accuracy: 0.9403\n",
            "Epoch 23/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2233 - accuracy: 0.9084\n",
            "Epoch 00023: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2234 - accuracy: 0.9083 - val_loss: 0.1655 - val_accuracy: 0.9369\n",
            "Epoch 24/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2261 - accuracy: 0.9061\n",
            "Epoch 00024: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.2258 - accuracy: 0.9062 - val_loss: 0.1569 - val_accuracy: 0.9413\n",
            "Epoch 25/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2308 - accuracy: 0.9052\n",
            "Epoch 00025: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2308 - accuracy: 0.9053 - val_loss: 0.1578 - val_accuracy: 0.9394\n",
            "Epoch 26/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9062\n",
            "Epoch 00026: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2271 - accuracy: 0.9062 - val_loss: 0.1688 - val_accuracy: 0.9312\n",
            "Epoch 27/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.9077\n",
            "Epoch 00027: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2282 - accuracy: 0.9077 - val_loss: 0.1817 - val_accuracy: 0.9297\n",
            "Epoch 28/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2238 - accuracy: 0.9091\n",
            "Epoch 00028: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2237 - accuracy: 0.9091 - val_loss: 0.1440 - val_accuracy: 0.9497\n",
            "Epoch 29/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2265 - accuracy: 0.9049\n",
            "Epoch 00029: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2263 - accuracy: 0.9050 - val_loss: 0.1636 - val_accuracy: 0.9359\n",
            "Epoch 30/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2261 - accuracy: 0.9099\n",
            "Epoch 00030: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2262 - accuracy: 0.9098 - val_loss: 0.1641 - val_accuracy: 0.9337\n",
            "Epoch 31/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9069\n",
            "Epoch 00031: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2275 - accuracy: 0.9069 - val_loss: 0.1604 - val_accuracy: 0.9341\n",
            "Epoch 32/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2243 - accuracy: 0.9086\n",
            "Epoch 00032: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 65s 216ms/step - loss: 0.2243 - accuracy: 0.9085 - val_loss: 0.1644 - val_accuracy: 0.9306\n",
            "Epoch 33/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2251 - accuracy: 0.9075\n",
            "Epoch 00033: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2249 - accuracy: 0.9076 - val_loss: 0.1621 - val_accuracy: 0.9378\n",
            "Epoch 34/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2277 - accuracy: 0.9064\n",
            "Epoch 00034: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2279 - accuracy: 0.9062 - val_loss: 0.1753 - val_accuracy: 0.9378\n",
            "Epoch 35/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2306 - accuracy: 0.9039\n",
            "Epoch 00035: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2310 - accuracy: 0.9037 - val_loss: 0.1755 - val_accuracy: 0.9281\n",
            "Epoch 36/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2244 - accuracy: 0.9090\n",
            "Epoch 00036: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2246 - accuracy: 0.9089 - val_loss: 0.1573 - val_accuracy: 0.9400\n",
            "Epoch 37/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9068\n",
            "Epoch 00037: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2248 - accuracy: 0.9067 - val_loss: 0.1568 - val_accuracy: 0.9388\n",
            "Epoch 38/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2236 - accuracy: 0.9088\n",
            "Epoch 00038: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2237 - accuracy: 0.9087 - val_loss: 0.1644 - val_accuracy: 0.9331\n",
            "Epoch 39/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2261 - accuracy: 0.9069\n",
            "Epoch 00039: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2261 - accuracy: 0.9068 - val_loss: 0.1522 - val_accuracy: 0.9406\n",
            "Epoch 40/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2237 - accuracy: 0.9081\n",
            "Epoch 00040: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2242 - accuracy: 0.9078 - val_loss: 0.1758 - val_accuracy: 0.9294\n",
            "Epoch 41/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9083\n",
            "Epoch 00041: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2247 - accuracy: 0.9083 - val_loss: 0.1659 - val_accuracy: 0.9337\n",
            "Epoch 42/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9067\n",
            "Epoch 00042: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2270 - accuracy: 0.9068 - val_loss: 0.1602 - val_accuracy: 0.9397\n",
            "Epoch 43/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2230 - accuracy: 0.9092\n",
            "Epoch 00043: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2229 - accuracy: 0.9091 - val_loss: 0.1473 - val_accuracy: 0.9425\n",
            "Epoch 44/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2168 - accuracy: 0.9122\n",
            "Epoch 00044: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2169 - accuracy: 0.9122 - val_loss: 0.1713 - val_accuracy: 0.9353\n",
            "Epoch 45/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2214 - accuracy: 0.9085\n",
            "Epoch 00045: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2213 - accuracy: 0.9086 - val_loss: 0.1572 - val_accuracy: 0.9381\n",
            "Epoch 46/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2244 - accuracy: 0.9088\n",
            "Epoch 00046: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2244 - accuracy: 0.9089 - val_loss: 0.1632 - val_accuracy: 0.9353\n",
            "Epoch 47/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2211 - accuracy: 0.9095\n",
            "Epoch 00047: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2209 - accuracy: 0.9095 - val_loss: 0.1652 - val_accuracy: 0.9353\n",
            "Epoch 48/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9100\n",
            "Epoch 00048: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2188 - accuracy: 0.9099 - val_loss: 0.1559 - val_accuracy: 0.9444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2208 - accuracy: 0.9096\n",
            "Epoch 00049: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.2208 - accuracy: 0.9097 - val_loss: 0.1392 - val_accuracy: 0.9494\n",
            "Epoch 50/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2224 - accuracy: 0.9085\n",
            "Epoch 00050: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2224 - accuracy: 0.9086 - val_loss: 0.1588 - val_accuracy: 0.9372\n",
            "Epoch 51/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2197 - accuracy: 0.9107\n",
            "Epoch 00051: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2196 - accuracy: 0.9106 - val_loss: 0.1641 - val_accuracy: 0.9369\n",
            "Epoch 52/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2226 - accuracy: 0.9107\n",
            "Epoch 00052: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2225 - accuracy: 0.9108 - val_loss: 0.1625 - val_accuracy: 0.9362\n",
            "Epoch 53/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2180 - accuracy: 0.9111\n",
            "Epoch 00053: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2181 - accuracy: 0.9111 - val_loss: 0.1549 - val_accuracy: 0.9400\n",
            "Epoch 54/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.9115\n",
            "Epoch 00054: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2177 - accuracy: 0.9115 - val_loss: 0.1642 - val_accuracy: 0.9362\n",
            "Epoch 55/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2222 - accuracy: 0.9111\n",
            "Epoch 00055: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.2223 - accuracy: 0.9110 - val_loss: 0.1707 - val_accuracy: 0.9378\n",
            "Epoch 56/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2189 - accuracy: 0.9101\n",
            "Epoch 00056: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2189 - accuracy: 0.9101 - val_loss: 0.1515 - val_accuracy: 0.9403\n",
            "Epoch 57/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2125 - accuracy: 0.9138\n",
            "Epoch 00057: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2125 - accuracy: 0.9138 - val_loss: 0.1751 - val_accuracy: 0.9328\n",
            "Epoch 58/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2128 - accuracy: 0.9130 ETA: 3s\n",
            "Epoch 00058: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2129 - accuracy: 0.9130 - val_loss: 0.1535 - val_accuracy: 0.9391\n",
            "Epoch 59/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2218 - accuracy: 0.9094\n",
            "Epoch 00059: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2218 - accuracy: 0.9094 - val_loss: 0.1599 - val_accuracy: 0.9369\n",
            "Epoch 60/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2184 - accuracy: 0.9100\n",
            "Epoch 00060: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2182 - accuracy: 0.9101 - val_loss: 0.1437 - val_accuracy: 0.9431\n",
            "Epoch 61/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2164 - accuracy: 0.9111\n",
            "Epoch 00061: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2167 - accuracy: 0.9111 - val_loss: 0.1602 - val_accuracy: 0.9394\n",
            "Epoch 62/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9133\n",
            "Epoch 00062: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2148 - accuracy: 0.9132 - val_loss: 0.1623 - val_accuracy: 0.9369\n",
            "Epoch 63/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2217 - accuracy: 0.9098\n",
            "Epoch 00063: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2217 - accuracy: 0.9098 - val_loss: 0.1576 - val_accuracy: 0.9444\n",
            "Epoch 64/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9127\n",
            "Epoch 00064: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2152 - accuracy: 0.9126 - val_loss: 0.1451 - val_accuracy: 0.9475\n",
            "Epoch 65/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2106 - accuracy: 0.9138\n",
            "Epoch 00065: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2109 - accuracy: 0.9136 - val_loss: 0.1536 - val_accuracy: 0.9397\n",
            "Epoch 66/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2159 - accuracy: 0.9114\n",
            "Epoch 00066: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2160 - accuracy: 0.9114 - val_loss: 0.1444 - val_accuracy: 0.9391\n",
            "Epoch 67/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2175 - accuracy: 0.9102\n",
            "Epoch 00067: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2176 - accuracy: 0.9102 - val_loss: 0.1607 - val_accuracy: 0.9362\n",
            "Epoch 68/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2122 - accuracy: 0.9142\n",
            "Epoch 00068: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2121 - accuracy: 0.9143 - val_loss: 0.1413 - val_accuracy: 0.9491\n",
            "Epoch 69/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2129 - accuracy: 0.9137\n",
            "Epoch 00069: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2130 - accuracy: 0.9136 - val_loss: 0.1524 - val_accuracy: 0.9388\n",
            "Epoch 70/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.9108\n",
            "Epoch 00070: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.2190 - accuracy: 0.9109 - val_loss: 0.1557 - val_accuracy: 0.9425\n",
            "Epoch 71/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2095 - accuracy: 0.9156\n",
            "Epoch 00071: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2096 - accuracy: 0.9155 - val_loss: 0.1561 - val_accuracy: 0.9444\n",
            "Epoch 72/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2119 - accuracy: 0.9135\n",
            "Epoch 00072: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2118 - accuracy: 0.9136 - val_loss: 0.1536 - val_accuracy: 0.9394\n",
            "Epoch 73/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2078 - accuracy: 0.9154\n",
            "Epoch 00073: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 212ms/step - loss: 0.2080 - accuracy: 0.9154 - val_loss: 0.1521 - val_accuracy: 0.9378\n",
            "Epoch 74/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2127 - accuracy: 0.9120\n",
            "Epoch 00074: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2127 - accuracy: 0.9118 - val_loss: 0.1598 - val_accuracy: 0.9375\n",
            "Epoch 75/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2085 - accuracy: 0.9139\n",
            "Epoch 00075: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2085 - accuracy: 0.9139 - val_loss: 0.1471 - val_accuracy: 0.9425\n",
            "Epoch 76/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2099 - accuracy: 0.9156\n",
            "Epoch 00076: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 65s 215ms/step - loss: 0.2098 - accuracy: 0.9157 - val_loss: 0.1442 - val_accuracy: 0.9456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 77/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2092 - accuracy: 0.9150\n",
            "Epoch 00077: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2091 - accuracy: 0.9150 - val_loss: 0.1366 - val_accuracy: 0.9484\n",
            "Epoch 78/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2078 - accuracy: 0.9154\n",
            "Epoch 00078: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2080 - accuracy: 0.9154 - val_loss: 0.1585 - val_accuracy: 0.9372\n",
            "Epoch 79/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2120 - accuracy: 0.9145\n",
            "Epoch 00079: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2119 - accuracy: 0.9145 - val_loss: 0.1488 - val_accuracy: 0.9413\n",
            "Epoch 80/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9160\n",
            "Epoch 00080: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2059 - accuracy: 0.9160 - val_loss: 0.1351 - val_accuracy: 0.9481\n",
            "Epoch 81/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.9144\n",
            "Epoch 00081: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2110 - accuracy: 0.9143 - val_loss: 0.1380 - val_accuracy: 0.9500\n",
            "Epoch 82/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2099 - accuracy: 0.9158\n",
            "Epoch 00082: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 212ms/step - loss: 0.2097 - accuracy: 0.9159 - val_loss: 0.1486 - val_accuracy: 0.9409\n",
            "Epoch 83/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.9177\n",
            "Epoch 00083: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2052 - accuracy: 0.9177 - val_loss: 0.1428 - val_accuracy: 0.9422\n",
            "Epoch 84/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2084 - accuracy: 0.9145\n",
            "Epoch 00084: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2086 - accuracy: 0.9144 - val_loss: 0.1474 - val_accuracy: 0.9447\n",
            "Epoch 85/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2102 - accuracy: 0.9140\n",
            "Epoch 00085: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2103 - accuracy: 0.9140 - val_loss: 0.1480 - val_accuracy: 0.9431\n",
            "Epoch 86/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2101 - accuracy: 0.9154\n",
            "Epoch 00086: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2100 - accuracy: 0.9154 - val_loss: 0.1588 - val_accuracy: 0.9384\n",
            "Epoch 87/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2111 - accuracy: 0.9149\n",
            "Epoch 00087: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2112 - accuracy: 0.9149 - val_loss: 0.1584 - val_accuracy: 0.9409\n",
            "Epoch 88/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2063 - accuracy: 0.9154\n",
            "Epoch 00088: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2062 - accuracy: 0.9153 - val_loss: 0.1504 - val_accuracy: 0.9403\n",
            "Epoch 89/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2065 - accuracy: 0.9164\n",
            "Epoch 00089: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2065 - accuracy: 0.9165 - val_loss: 0.1488 - val_accuracy: 0.9413\n",
            "Epoch 90/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2087 - accuracy: 0.9163\n",
            "Epoch 00090: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2085 - accuracy: 0.9164 - val_loss: 0.1553 - val_accuracy: 0.9400\n",
            "Epoch 91/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2073 - accuracy: 0.9168\n",
            "Epoch 00091: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2077 - accuracy: 0.9166 - val_loss: 0.1507 - val_accuracy: 0.9391\n",
            "Epoch 92/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2070 - accuracy: 0.9162\n",
            "Epoch 00092: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2074 - accuracy: 0.9160 - val_loss: 0.1589 - val_accuracy: 0.9384\n",
            "Epoch 93/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2074 - accuracy: 0.9164\n",
            "Epoch 00093: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2070 - accuracy: 0.9166 - val_loss: 0.1539 - val_accuracy: 0.9366\n",
            "Epoch 94/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9167\n",
            "Epoch 00094: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2058 - accuracy: 0.9167 - val_loss: 0.1387 - val_accuracy: 0.9469\n",
            "Epoch 95/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9186\n",
            "Epoch 00095: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.2006 - accuracy: 0.9185 - val_loss: 0.1376 - val_accuracy: 0.9494\n",
            "Epoch 96/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9159\n",
            "Epoch 00096: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 215ms/step - loss: 0.2079 - accuracy: 0.9160 - val_loss: 0.1474 - val_accuracy: 0.9413\n",
            "Epoch 97/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9188\n",
            "Epoch 00097: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2005 - accuracy: 0.9189 - val_loss: 0.1398 - val_accuracy: 0.9469\n",
            "Epoch 98/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2031 - accuracy: 0.9193\n",
            "Epoch 00098: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2032 - accuracy: 0.9192 - val_loss: 0.1460 - val_accuracy: 0.9478\n",
            "Epoch 99/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2029 - accuracy: 0.9166\n",
            "Epoch 00099: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2025 - accuracy: 0.9168 - val_loss: 0.1567 - val_accuracy: 0.9362\n",
            "Epoch 100/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2025 - accuracy: 0.9174\n",
            "Epoch 00100: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2024 - accuracy: 0.9175 - val_loss: 0.1501 - val_accuracy: 0.9438\n",
            "Epoch 101/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2039 - accuracy: 0.9167\n",
            "Epoch 00101: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2039 - accuracy: 0.9168 - val_loss: 0.1484 - val_accuracy: 0.9394\n",
            "Epoch 102/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1999 - accuracy: 0.9191\n",
            "Epoch 00102: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1999 - accuracy: 0.9191 - val_loss: 0.1375 - val_accuracy: 0.9472\n",
            "Epoch 103/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9200\n",
            "Epoch 00103: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1996 - accuracy: 0.9199 - val_loss: 0.1381 - val_accuracy: 0.9469\n",
            "Epoch 104/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2018 - accuracy: 0.9178\n",
            "Epoch 00104: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2018 - accuracy: 0.9178 - val_loss: 0.1462 - val_accuracy: 0.9431\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 105/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1975 - accuracy: 0.9197\n",
            "Epoch 00105: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1972 - accuracy: 0.9198 - val_loss: 0.1362 - val_accuracy: 0.9491\n",
            "Epoch 106/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1962 - accuracy: 0.9206\n",
            "Epoch 00106: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1962 - accuracy: 0.9206 - val_loss: 0.1421 - val_accuracy: 0.9491\n",
            "Epoch 107/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1963 - accuracy: 0.9211\n",
            "Epoch 00107: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1964 - accuracy: 0.9210 - val_loss: 0.1409 - val_accuracy: 0.9447\n",
            "Epoch 108/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1998 - accuracy: 0.9195\n",
            "Epoch 00108: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1999 - accuracy: 0.9194 - val_loss: 0.1425 - val_accuracy: 0.9431\n",
            "Epoch 109/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1993 - accuracy: 0.9186\n",
            "Epoch 00109: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 215ms/step - loss: 0.1991 - accuracy: 0.9186 - val_loss: 0.1482 - val_accuracy: 0.9419\n",
            "Epoch 110/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2034 - accuracy: 0.9173\n",
            "Epoch 00110: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 215ms/step - loss: 0.2032 - accuracy: 0.9174 - val_loss: 0.1363 - val_accuracy: 0.9478\n",
            "Epoch 111/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1986 - accuracy: 0.9187\n",
            "Epoch 00111: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1987 - accuracy: 0.9186 - val_loss: 0.1443 - val_accuracy: 0.9434\n",
            "Epoch 112/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9164\n",
            "Epoch 00112: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2006 - accuracy: 0.9165 - val_loss: 0.1384 - val_accuracy: 0.9434\n",
            "Epoch 113/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1969 - accuracy: 0.9206\n",
            "Epoch 00113: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1968 - accuracy: 0.9207 - val_loss: 0.1327 - val_accuracy: 0.9491\n",
            "Epoch 114/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.9191\n",
            "Epoch 00114: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2021 - accuracy: 0.9191 - val_loss: 0.1389 - val_accuracy: 0.9481\n",
            "Epoch 115/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1992 - accuracy: 0.9185\n",
            "Epoch 00115: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1993 - accuracy: 0.9185 - val_loss: 0.1402 - val_accuracy: 0.9431\n",
            "Epoch 116/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2008 - accuracy: 0.9190\n",
            "Epoch 00116: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2008 - accuracy: 0.9190 - val_loss: 0.1394 - val_accuracy: 0.9472\n",
            "Epoch 117/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2008 - accuracy: 0.9184\n",
            "Epoch 00117: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.2010 - accuracy: 0.9183 - val_loss: 0.1350 - val_accuracy: 0.9516\n",
            "Epoch 118/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1952 - accuracy: 0.9214\n",
            "Epoch 00118: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1954 - accuracy: 0.9214 - val_loss: 0.1336 - val_accuracy: 0.9506\n",
            "Epoch 119/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2012 - accuracy: 0.9179\n",
            "Epoch 00119: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2010 - accuracy: 0.9180 - val_loss: 0.1346 - val_accuracy: 0.9491\n",
            "Epoch 120/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9193\n",
            "Epoch 00120: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2001 - accuracy: 0.9194 - val_loss: 0.1430 - val_accuracy: 0.9459\n",
            "Epoch 121/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1997 - accuracy: 0.9193\n",
            "Epoch 00121: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1998 - accuracy: 0.9192 - val_loss: 0.1433 - val_accuracy: 0.9484\n",
            "Epoch 122/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9199\n",
            "Epoch 00122: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1974 - accuracy: 0.9199 - val_loss: 0.1410 - val_accuracy: 0.9444\n",
            "Epoch 123/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1957 - accuracy: 0.9190\n",
            "Epoch 00123: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 215ms/step - loss: 0.1959 - accuracy: 0.9188 - val_loss: 0.1297 - val_accuracy: 0.9550\n",
            "Epoch 124/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9193\n",
            "Epoch 00124: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.2023 - accuracy: 0.9192 - val_loss: 0.1390 - val_accuracy: 0.9459\n",
            "Epoch 125/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9232\n",
            "Epoch 00125: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1941 - accuracy: 0.9232 - val_loss: 0.1397 - val_accuracy: 0.9450\n",
            "Epoch 126/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1955 - accuracy: 0.9219\n",
            "Epoch 00126: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1953 - accuracy: 0.9220 - val_loss: 0.1274 - val_accuracy: 0.9506\n",
            "Epoch 127/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1914 - accuracy: 0.9231\n",
            "Epoch 00127: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1914 - accuracy: 0.9231 - val_loss: 0.1393 - val_accuracy: 0.9453\n",
            "Epoch 128/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9226\n",
            "Epoch 00128: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1912 - accuracy: 0.9224 - val_loss: 0.1525 - val_accuracy: 0.9406\n",
            "Epoch 129/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1992 - accuracy: 0.9197\n",
            "Epoch 00129: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1992 - accuracy: 0.9197 - val_loss: 0.1555 - val_accuracy: 0.9388\n",
            "Epoch 130/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1917 - accuracy: 0.9241\n",
            "Epoch 00130: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1914 - accuracy: 0.9243 - val_loss: 0.1364 - val_accuracy: 0.9475\n",
            "Epoch 131/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1969 - accuracy: 0.9206\n",
            "Epoch 00131: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1968 - accuracy: 0.9206 - val_loss: 0.1331 - val_accuracy: 0.9509\n",
            "Epoch 132/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1991 - accuracy: 0.9203\n",
            "Epoch 00132: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1990 - accuracy: 0.9204 - val_loss: 0.1272 - val_accuracy: 0.9531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 133/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1915 - accuracy: 0.9236\n",
            "Epoch 00133: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1915 - accuracy: 0.9236 - val_loss: 0.1489 - val_accuracy: 0.9409\n",
            "Epoch 134/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1901 - accuracy: 0.9239\n",
            "Epoch 00134: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1904 - accuracy: 0.9237 - val_loss: 0.1312 - val_accuracy: 0.9538\n",
            "Epoch 135/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1936 - accuracy: 0.9231\n",
            "Epoch 00135: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1938 - accuracy: 0.9230 - val_loss: 0.1401 - val_accuracy: 0.9478\n",
            "Epoch 136/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1928 - accuracy: 0.9235\n",
            "Epoch 00136: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1926 - accuracy: 0.9236 - val_loss: 0.1306 - val_accuracy: 0.9491\n",
            "Epoch 137/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1886 - accuracy: 0.9241\n",
            "Epoch 00137: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1886 - accuracy: 0.9241 - val_loss: 0.1272 - val_accuracy: 0.9519\n",
            "Epoch 138/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9254\n",
            "Epoch 00138: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 65s 215ms/step - loss: 0.1880 - accuracy: 0.9254 - val_loss: 0.1369 - val_accuracy: 0.9444\n",
            "Epoch 139/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1927 - accuracy: 0.9236\n",
            "Epoch 00139: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1927 - accuracy: 0.9237 - val_loss: 0.1432 - val_accuracy: 0.9428\n",
            "Epoch 140/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1913 - accuracy: 0.9227\n",
            "Epoch 00140: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1914 - accuracy: 0.9228 - val_loss: 0.1349 - val_accuracy: 0.9453\n",
            "Epoch 141/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9235\n",
            "Epoch 00141: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1877 - accuracy: 0.9235 - val_loss: 0.1374 - val_accuracy: 0.9438\n",
            "Epoch 142/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9208\n",
            "Epoch 00142: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 65s 217ms/step - loss: 0.1972 - accuracy: 0.9208 - val_loss: 0.1303 - val_accuracy: 0.9488\n",
            "Epoch 143/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9226\n",
            "Epoch 00143: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 215ms/step - loss: 0.1893 - accuracy: 0.9226 - val_loss: 0.1292 - val_accuracy: 0.9506\n",
            "Epoch 144/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1875 - accuracy: 0.9234\n",
            "Epoch 00144: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 65s 217ms/step - loss: 0.1874 - accuracy: 0.9234 - val_loss: 0.1400 - val_accuracy: 0.9463\n",
            "Epoch 145/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9208\n",
            "Epoch 00145: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1961 - accuracy: 0.9208 - val_loss: 0.1375 - val_accuracy: 0.9428\n",
            "Epoch 146/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9227\n",
            "Epoch 00146: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1934 - accuracy: 0.9226 - val_loss: 0.1314 - val_accuracy: 0.9481\n",
            "Epoch 147/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9221\n",
            "Epoch 00147: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1927 - accuracy: 0.9221 - val_loss: 0.1326 - val_accuracy: 0.9522\n",
            "Epoch 148/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1887 - accuracy: 0.9248\n",
            "Epoch 00148: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1885 - accuracy: 0.9249 - val_loss: 0.1278 - val_accuracy: 0.9522\n",
            "Epoch 149/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1875 - accuracy: 0.9245\n",
            "Epoch 00149: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1876 - accuracy: 0.9243 - val_loss: 0.1322 - val_accuracy: 0.9497\n",
            "Epoch 150/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1901 - accuracy: 0.9239\n",
            "Epoch 00150: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1902 - accuracy: 0.9238 - val_loss: 0.1326 - val_accuracy: 0.9459\n",
            "Epoch 151/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1887 - accuracy: 0.9246\n",
            "Epoch 00151: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1889 - accuracy: 0.9245 - val_loss: 0.1374 - val_accuracy: 0.9500\n",
            "Epoch 152/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9231\n",
            "Epoch 00152: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1931 - accuracy: 0.9230 - val_loss: 0.1311 - val_accuracy: 0.9469\n",
            "Epoch 153/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1885 - accuracy: 0.9237\n",
            "Epoch 00153: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1886 - accuracy: 0.9236 - val_loss: 0.1424 - val_accuracy: 0.9463\n",
            "Epoch 154/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1897 - accuracy: 0.9242\n",
            "Epoch 00154: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1897 - accuracy: 0.9241 - val_loss: 0.1263 - val_accuracy: 0.9531\n",
            "Epoch 155/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1828 - accuracy: 0.9256\n",
            "Epoch 00155: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1828 - accuracy: 0.9256 - val_loss: 0.1208 - val_accuracy: 0.9516\n",
            "Epoch 156/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1912 - accuracy: 0.9235\n",
            "Epoch 00156: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1913 - accuracy: 0.9236 - val_loss: 0.1230 - val_accuracy: 0.9531\n",
            "Epoch 157/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1917 - accuracy: 0.9228\n",
            "Epoch 00157: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1916 - accuracy: 0.9228 - val_loss: 0.1324 - val_accuracy: 0.9509\n",
            "Epoch 158/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1861 - accuracy: 0.9252\n",
            "Epoch 00158: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1861 - accuracy: 0.9252 - val_loss: 0.1270 - val_accuracy: 0.9484\n",
            "Epoch 159/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1872 - accuracy: 0.9266\n",
            "Epoch 00159: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1876 - accuracy: 0.9264 - val_loss: 0.1242 - val_accuracy: 0.9541\n",
            "Epoch 160/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1896 - accuracy: 0.9243\n",
            "Epoch 00160: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1896 - accuracy: 0.9242 - val_loss: 0.1372 - val_accuracy: 0.9441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 161/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1862 - accuracy: 0.9264\n",
            "Epoch 00161: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1861 - accuracy: 0.9264 - val_loss: 0.1340 - val_accuracy: 0.9509\n",
            "Epoch 162/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1894 - accuracy: 0.9230\n",
            "Epoch 00162: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1894 - accuracy: 0.9230 - val_loss: 0.1375 - val_accuracy: 0.9463\n",
            "Epoch 163/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9263\n",
            "Epoch 00163: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1820 - accuracy: 0.9263 - val_loss: 0.1137 - val_accuracy: 0.9613\n",
            "Epoch 164/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1908 - accuracy: 0.9229\n",
            "Epoch 00164: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1909 - accuracy: 0.9228 - val_loss: 0.1231 - val_accuracy: 0.9541\n",
            "Epoch 165/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1882 - accuracy: 0.9250\n",
            "Epoch 00165: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1880 - accuracy: 0.9251 - val_loss: 0.1250 - val_accuracy: 0.9522\n",
            "Epoch 166/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.9258\n",
            "Epoch 00166: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1837 - accuracy: 0.9257 - val_loss: 0.1290 - val_accuracy: 0.9509\n",
            "Epoch 167/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1832 - accuracy: 0.9260\n",
            "Epoch 00167: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 212ms/step - loss: 0.1833 - accuracy: 0.9260 - val_loss: 0.1383 - val_accuracy: 0.9469\n",
            "Epoch 168/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1848 - accuracy: 0.9273\n",
            "Epoch 00168: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1848 - accuracy: 0.9272 - val_loss: 0.1290 - val_accuracy: 0.9491\n",
            "Epoch 169/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9266\n",
            "Epoch 00169: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1844 - accuracy: 0.9265 - val_loss: 0.1321 - val_accuracy: 0.9494\n",
            "Epoch 170/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1837 - accuracy: 0.9271\n",
            "Epoch 00170: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1837 - accuracy: 0.9271 - val_loss: 0.1335 - val_accuracy: 0.9469\n",
            "Epoch 171/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1896 - accuracy: 0.9243\n",
            "Epoch 00171: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 212ms/step - loss: 0.1896 - accuracy: 0.9243 - val_loss: 0.1193 - val_accuracy: 0.9544\n",
            "Epoch 172/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1863 - accuracy: 0.9262\n",
            "Epoch 00172: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1861 - accuracy: 0.9262 - val_loss: 0.1295 - val_accuracy: 0.9528\n",
            "Epoch 173/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1903 - accuracy: 0.9243\n",
            "Epoch 00173: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1903 - accuracy: 0.9243 - val_loss: 0.1233 - val_accuracy: 0.9509\n",
            "Epoch 174/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9266\n",
            "Epoch 00174: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1847 - accuracy: 0.9267 - val_loss: 0.1165 - val_accuracy: 0.9531\n",
            "Epoch 175/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9285\n",
            "Epoch 00175: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1798 - accuracy: 0.9284 - val_loss: 0.1273 - val_accuracy: 0.9497\n",
            "Epoch 176/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1893 - accuracy: 0.9240\n",
            "Epoch 00176: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1894 - accuracy: 0.9239 - val_loss: 0.1386 - val_accuracy: 0.9466\n",
            "Epoch 177/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.9278\n",
            "Epoch 00177: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1815 - accuracy: 0.9279 - val_loss: 0.1306 - val_accuracy: 0.9497\n",
            "Epoch 178/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1808 - accuracy: 0.9278\n",
            "Epoch 00178: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1809 - accuracy: 0.9277 - val_loss: 0.1222 - val_accuracy: 0.9525\n",
            "Epoch 179/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1843 - accuracy: 0.9260\n",
            "Epoch 00179: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1842 - accuracy: 0.9261 - val_loss: 0.1288 - val_accuracy: 0.9513\n",
            "Epoch 180/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1841 - accuracy: 0.9259\n",
            "Epoch 00180: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 212ms/step - loss: 0.1839 - accuracy: 0.9259 - val_loss: 0.1326 - val_accuracy: 0.9463\n",
            "Epoch 181/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.9245\n",
            "Epoch 00181: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1876 - accuracy: 0.9244 - val_loss: 0.1310 - val_accuracy: 0.9509\n",
            "Epoch 182/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1813 - accuracy: 0.9290\n",
            "Epoch 00182: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1813 - accuracy: 0.9290 - val_loss: 0.1203 - val_accuracy: 0.9553\n",
            "Epoch 183/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1798 - accuracy: 0.9278\n",
            "Epoch 00183: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1798 - accuracy: 0.9278 - val_loss: 0.1271 - val_accuracy: 0.9500\n",
            "Epoch 184/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1834 - accuracy: 0.9258\n",
            "Epoch 00184: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1835 - accuracy: 0.9258 - val_loss: 0.1230 - val_accuracy: 0.9534\n",
            "Epoch 185/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9279\n",
            "Epoch 00185: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 210ms/step - loss: 0.1791 - accuracy: 0.9278 - val_loss: 0.1214 - val_accuracy: 0.9500\n",
            "Epoch 186/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1791 - accuracy: 0.9293\n",
            "Epoch 00186: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1791 - accuracy: 0.9294 - val_loss: 0.1217 - val_accuracy: 0.9563\n",
            "Epoch 187/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1778 - accuracy: 0.9291\n",
            "Epoch 00187: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1776 - accuracy: 0.9292 - val_loss: 0.1419 - val_accuracy: 0.9459\n",
            "Epoch 188/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1762 - accuracy: 0.9298\n",
            "Epoch 00188: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1763 - accuracy: 0.9297 - val_loss: 0.1189 - val_accuracy: 0.9566\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 189/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.9284\n",
            "Epoch 00189: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 212ms/step - loss: 0.1820 - accuracy: 0.9285 - val_loss: 0.1396 - val_accuracy: 0.9456\n",
            "Epoch 190/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1808 - accuracy: 0.9276\n",
            "Epoch 00190: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 65s 216ms/step - loss: 0.1810 - accuracy: 0.9276 - val_loss: 0.1342 - val_accuracy: 0.9500\n",
            "Epoch 191/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1806 - accuracy: 0.9276\n",
            "Epoch 00191: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1806 - accuracy: 0.9276 - val_loss: 0.1179 - val_accuracy: 0.9588\n",
            "Epoch 192/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9280\n",
            "Epoch 00192: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1818 - accuracy: 0.9280 - val_loss: 0.1176 - val_accuracy: 0.9525\n",
            "Epoch 193/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9294\n",
            "Epoch 00193: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1790 - accuracy: 0.9295 - val_loss: 0.1235 - val_accuracy: 0.9547\n",
            "Epoch 194/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1813 - accuracy: 0.9267\n",
            "Epoch 00194: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1811 - accuracy: 0.9268 - val_loss: 0.1091 - val_accuracy: 0.9622\n",
            "Epoch 195/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1802 - accuracy: 0.9286\n",
            "Epoch 00195: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1802 - accuracy: 0.9287 - val_loss: 0.1231 - val_accuracy: 0.9569\n",
            "Epoch 196/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1779 - accuracy: 0.9290\n",
            "Epoch 00196: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 208ms/step - loss: 0.1776 - accuracy: 0.9291 - val_loss: 0.1237 - val_accuracy: 0.9559\n",
            "Epoch 197/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1805 - accuracy: 0.9262\n",
            "Epoch 00197: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1806 - accuracy: 0.9261 - val_loss: 0.1371 - val_accuracy: 0.9466\n",
            "Epoch 198/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9262\n",
            "Epoch 00198: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 210ms/step - loss: 0.1820 - accuracy: 0.9262 - val_loss: 0.1174 - val_accuracy: 0.9531\n",
            "Epoch 199/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9297\n",
            "Epoch 00199: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1763 - accuracy: 0.9297 - val_loss: 0.1277 - val_accuracy: 0.9513\n",
            "Epoch 200/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1771 - accuracy: 0.9285\n",
            "Epoch 00200: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1770 - accuracy: 0.9286 - val_loss: 0.1249 - val_accuracy: 0.9469\n",
            "Epoch 201/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1773 - accuracy: 0.9297\n",
            "Epoch 00201: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1775 - accuracy: 0.9296 - val_loss: 0.1184 - val_accuracy: 0.9522\n",
            "Epoch 202/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1803 - accuracy: 0.9275\n",
            "Epoch 00202: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1802 - accuracy: 0.9275 - val_loss: 0.1272 - val_accuracy: 0.9509\n",
            "Epoch 203/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1804 - accuracy: 0.9279\n",
            "Epoch 00203: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1803 - accuracy: 0.9279 - val_loss: 0.1251 - val_accuracy: 0.9544\n",
            "Epoch 204/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1757 - accuracy: 0.9297\n",
            "Epoch 00204: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1757 - accuracy: 0.9296 - val_loss: 0.1331 - val_accuracy: 0.9450\n",
            "Epoch 205/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9256\n",
            "Epoch 00205: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 210ms/step - loss: 0.1847 - accuracy: 0.9256 - val_loss: 0.1185 - val_accuracy: 0.9569\n",
            "Epoch 206/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1803 - accuracy: 0.9286\n",
            "Epoch 00206: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 210ms/step - loss: 0.1802 - accuracy: 0.9287 - val_loss: 0.1167 - val_accuracy: 0.9566\n",
            "Epoch 207/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1817 - accuracy: 0.9270\n",
            "Epoch 00207: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1817 - accuracy: 0.9270 - val_loss: 0.1220 - val_accuracy: 0.9559\n",
            "Epoch 208/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1769 - accuracy: 0.9290\n",
            "Epoch 00208: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 211ms/step - loss: 0.1770 - accuracy: 0.9290 - val_loss: 0.1179 - val_accuracy: 0.9559\n",
            "Epoch 209/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1747 - accuracy: 0.9308\n",
            "Epoch 00209: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1745 - accuracy: 0.9308 - val_loss: 0.1137 - val_accuracy: 0.9553\n",
            "Epoch 210/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1796 - accuracy: 0.9293\n",
            "Epoch 00210: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 215ms/step - loss: 0.1796 - accuracy: 0.9294 - val_loss: 0.1243 - val_accuracy: 0.9528\n",
            "Epoch 211/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1785 - accuracy: 0.9303\n",
            "Epoch 00211: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1783 - accuracy: 0.9303 - val_loss: 0.1244 - val_accuracy: 0.9528\n",
            "Epoch 212/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9310\n",
            "Epoch 00212: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 66s 219ms/step - loss: 0.1782 - accuracy: 0.9309 - val_loss: 0.1252 - val_accuracy: 0.9534\n",
            "Epoch 213/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1746 - accuracy: 0.9307\n",
            "Epoch 00213: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 213ms/step - loss: 0.1745 - accuracy: 0.9308 - val_loss: 0.1226 - val_accuracy: 0.9522\n",
            "Epoch 214/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1746 - accuracy: 0.9307\n",
            "Epoch 00214: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 212ms/step - loss: 0.1744 - accuracy: 0.9307 - val_loss: 0.1314 - val_accuracy: 0.9494\n",
            "Epoch 215/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9310\n",
            "Epoch 00215: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 65s 215ms/step - loss: 0.1743 - accuracy: 0.9310 - val_loss: 0.1313 - val_accuracy: 0.9525\n",
            "Epoch 216/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9297\n",
            "Epoch 00216: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1745 - accuracy: 0.9296 - val_loss: 0.1079 - val_accuracy: 0.9625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 217/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1770 - accuracy: 0.9290\n",
            "Epoch 00217: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 214ms/step - loss: 0.1769 - accuracy: 0.9291 - val_loss: 0.1269 - val_accuracy: 0.9528\n",
            "Epoch 218/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1713 - accuracy: 0.9327\n",
            "Epoch 00218: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1713 - accuracy: 0.9327 - val_loss: 0.1196 - val_accuracy: 0.9544\n",
            "Epoch 219/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.9309\n",
            "Epoch 00219: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 64s 212ms/step - loss: 0.1710 - accuracy: 0.9309 - val_loss: 0.1325 - val_accuracy: 0.9497\n",
            "Epoch 220/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1732 - accuracy: 0.9319\n",
            "Epoch 00220: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 210ms/step - loss: 0.1732 - accuracy: 0.9318 - val_loss: 0.1350 - val_accuracy: 0.9466\n",
            "Epoch 221/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9294\n",
            "Epoch 00221: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 210ms/step - loss: 0.1749 - accuracy: 0.9296 - val_loss: 0.1085 - val_accuracy: 0.9581\n",
            "Epoch 222/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9285\n",
            "Epoch 00222: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 209ms/step - loss: 0.1769 - accuracy: 0.9284 - val_loss: 0.1312 - val_accuracy: 0.9528\n",
            "Epoch 223/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9280\n",
            "Epoch 00223: saving model to ./trained_model/model_checkpoint\n",
            "300/300 [==============================] - 63s 208ms/step - loss: 0.1797 - accuracy: 0.9280 - val_loss: 0.1205 - val_accuracy: 0.9550\n",
            "Epoch 224/1000\n",
            "299/300 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9302"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY67TryWSb9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertIRSystem(IRSystem):\n",
        "    def __init__(self):\n",
        "        self.documents = list(documents.values())\n",
        "        self.corpus = [flatten_text(doc.preprocessed) for doc in self.documents]\n",
        "        self.bm25 = BM25Plus(self.corpus)\n",
        "\n",
        "        \n",
        "    def first_stage_ranking(self, preprocessed_query):\n",
        "        scores = self.bm25.get_scores(preprocessed_query.preprocessed[0])\n",
        "        doc_list_ids = np.argsort(scores)[::-1]\n",
        "        return np.take(self.documents, doc_list_ids, axis=0).tolist()\n",
        "\n",
        "                \n",
        "    def search(self, query):\n",
        "        preprocessed_query = queries[query.query_id]\n",
        "        fs_ranked_docs = self.first_stage_ranking(preprocessed_query)\n",
        "                \n",
        "        reranked_docs = rerank_docs_based_on_query(preprocessed_query, fs_ranked_docs)\n",
        "            \n",
        "        return reranked_docs  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHCd85-vSb9O",
        "colab_type": "code",
        "colab": {},
        "outputId": "235e19b7-716c-47a8-dc02-259fb992c870"
      },
      "source": [
        "mean_average_precision(BertIRSystem(), submit_result=False, author_name=\"Kaplenko, Mykola\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean average precision: 42.740% \n",
            "Not submitted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2QkPE31Sb9V",
        "colab_type": "text"
      },
      "source": [
        "<br><br>\n",
        "<p style=\"font-size:120%;\">Note the increase in MAP after the document reranking by our second-stage model. The increase is not too high but:<br/><br/>\n",
        "<ul style=\"font-size:120%;\">\n",
        "<li>The reranking was applied only once to pairs of neighbouring documents (to achieve the reasonable query response time). The higher number of pairwise rerankings significantly slows down the document retrieval</li><br/>\n",
        "<li>Model was trained from scratch. We did no fine-tuning and did not use any huge model pretrained by <i>Google</i> on <i>gigabytes</i> of data</li><br/> \n",
        "<li>Our model was trained on general language understanding task (i.e. document prediction based on extracted words) and have never used the relevance labels. In contrast, the real information retrieval systems always use the extreamly huge datasets of queries and the human-defined document relevance labels</li></ul>\n",
        "<br>\n",
        "<p style=\"font-size:120%;\">It will be interesting to change the project slightly in the following years by providing the train and the validation dataset such that we can train our models for the primary tasks they will be used for.</p><br>\n",
        "<hr>\n",
        "\n",
        "<br/>\n",
        "<p align=\"center\" style=\"font-size:120%;text-align:center;\">Thank you for <i>attention</i>. I hope you have enjoyed our small information retrieval challenge.<br/><br>"
      ]
    }
  ]
}